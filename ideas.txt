

Conference Paper starting points :
  Hinton et al : Grammar as a Foreign Language
    http://arxiv.org/abs/1412.7449v2  :: [v2] Sat, 28 Feb 2015 03:16:54 GMT (115kb)
    Google-2015_Grammar-as-Foreign-Langauge_1412.7449v2.pdf

  Collobert  (Possible equivalent performance-wise to Berkeley parser)
    Natural Language from (almost) Scratch :
      http://ronan.collobert.com/pub/matos/2011_nlp_jmlr.pdf
    Code (Non-Commercial)
      http://ronan.collobert.com/senna/
    Neural Net version (Cython) (documented, complete with NER examples)
      https://github.com/erickrf/nlpnet
    Neural Net version (Theano) (minimal)
      https://github.com/Fematich/nn_ner
      
  Stanford course 
    Socher : cs224d
      http://cs224d.stanford.edu/syllabus.html    

  Theano LSTM: 
    http://www.sphere-engineering.com/blog/quickanswers-io-a-new-algorithm.html
      https://github.com/keyua-cisco/theano-nlp
    Shawn Tan (NUS...)
      https://github.com/shawntan?tab=repositories
      https://blog.wtf.sg/about/
  
  spaCy: --AGPL-- - Now MIT
    Same basic engine as the 500-lines blog post (which I've already trans-coded)
    Docs
      http://honnibal.github.io/spaCy/
    Code
      https://github.com/honnibal/spaCy
    NER : 
      13/04: Version 0.80 released. Includes named entity recognition, better sentence boundary detection, and many bug fixes.
      https://github.com/honnibal/spaCy/issues/62

  DEPENDENCY-BASED WORD EMBEDDINGS
    Use dependency-derived contexts to imply the word embeddings
      https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/
    
  Structural inference
    GTN
      Bottou-1997_GraphTransformerNetworks_gtn.pdf
    ParseTrees
      Collobert-2011_BuildingParseTrees_aistats.pdf
      Word embeddings initialised from previous work (helpful)
      Produced likelihoods of each tag at every position
      Viterbi for producing best sequence of symbols
      Contraints so that tree conditions satisfied
    Recursive NNs (Socher)
      http://cs224d.stanford.edu/lectures/CS224d-Lecture9.pdf
        Binarized trees sufficient for language parsing (up to a post-processing step)
        Also : PCFG hints for better tree 'tree pre-pruning' (but that requires pre-labelled training data too)
      https://www.youtube.com/watch?v=DJHvaGU9SW8
    
    Long Short-Term Memory Over Tree Structures  
      http://arxiv.org/abs/1503.04881     
    or Tai? in Socher's lab...  Submitted to ACML
 
    
    
 
 
  Check out 
    Socher : MV-RNN for Relationship Classification (looks good for Handshakes...)
    Socher : Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank 2013
  
-------------------  
Conference Paper ideas / starting points :

  Scheme for : "Middle out" Learning of Grammar Embeddings
    Learn word-vectors in the standard way, with left- and right-contexts being trained to 
      imply the middle 'idea' (which will initially just be the word, but will later be an 'idea')
      This will train the word vectors (if they are used, or the previous level's combiner and ancestors) 
        and the current context transformer
      
    Also, over the sentence, combine a pair together (using a combiner transformer), and
      do the same imputation for the middle 'idea' (this is the middle out piece)
      This will train the combiner transformer
      
    Looking at the whole sentence, pick the combiner error with the lowest value (i.e. most predicable idea in context)
      [ alternatively, for the null hypothesis, just pick a pair at random ]
      and replace that pair with the (true) result 
      And apply the procedure again, until left with one vector (the sentence 'idea')
        At which point, end

    
    Test word vectors with diagram of embedding learned (null should not care(?))
    
    See whether grammar has been learned by training a simple POS classifier on 'idea' vectors vs null
      But POS only actually labels word 'ideas'.  
      And these are trained to be insensitive to the current context (i.e. always the same, independent of the current sentence)
      So, in what sense would grammar get adsorbed into the word vectors?
        The tree-ification would have to provide additional information over-and-above the linear structure / local context
        How could that piece be fed as an extra feature alongside the word vectors?
          Perhaps as a simple indicator as to whether the word becomes a left or a right leaf?
            But a lot of that associativity is built into the linear structure already
              Information would have to indicate higher-order associations
              A lot of work would have to be done before this glimmer of light could be unveiled...
    
    Test to see whether parse trees make sense vs null too

    
    ... Time to figure out what 'simple scheme' for the context transformer
    ... (and then the combiner transformer too)
    
    
    Simplest route is to play with Collobert models

      Neural Net version (Cython) (documented, complete with NER examples)
        https://github.com/erickrf/nlpnet
      Neural Net version (Theano) (minimal)
        https://github.com/Fematich/nn_ner

      http://matpalm.com/blog/2015/03/28/theano_word_embeddings/

    Add curruculum idea too
    And cite Klein Manning 2001
    
    What would it mean to train LSTM on sentences 'unsupervised'?
-------------------
  
Alternative : 

  DONE 
    Create Entity extraction corpuses from output of :
      Stanford NER
      Berkeley NER

    Then train LSTM network on those corpuses

    Requires out-of-sample validation set 
      and common test sets
    
    Corpus to use : CoNLL-2003, which has NER training, test and 'raw' sets 
      (as well as 1Gb of additional corpus data)
    
    See whether network can (a) replicate and (b) supercede 
    
    Secondary goal : 
      Reduce need to rely on anti-commercial licensed software
    
    Other avenues: 
      Curriculum training based on (say) sentence length
        Or degree of agreement from corpus builders

      Additional NER methods, so that can do voting, and ensembling

    
  DONE :: ICONIP 2016 ? (was ICLR)
    Binary re-representation of word embeddings

    
  Idea :
    Geometry for word analogies, tailored for sparse vectors
      X:Y as A:?
        ? = A+(Y-X)  # clearly
        ? = Y+(A-X)  # in dense space
        ? = (A+Y) -X # in dense space
      
      but want to think only about non-negative 'things'
        need to set up REPL fns to quickly sort through combinations / filters / conjunctions, etc

      regular geometry:
        >>> closest_to(man)
        man, woman, girl, person, men, teenager, she, friend, he, father, her, boy, someone, mother, him, his, victim, son, who, guy
        >>> closest_to(woman)
        woman, man, girl, mother, teenager, daughter, wife, women, her, person, she, girlfriend, friend, men, husband, widow, couple, boy, someone, victim

        >>> closest_to(king)
        king, queen, henry, mswati, mongkut, eirik, charles, vajiravudh, thoden, wenceslaus, zvonimir, athelstan, vladislaus, thelred, gojong, prince, jayavarman, kalkaua, sweyn, pomare
        >>> closest_to(queen)
        queen, princess, elizabeth, king, margrethe, empress, lady, sister, prince, sirikit, mary, cixi, monarch, daughter, duchess, olten, mother, infanta, rania, widow

        >>> count_non_zero(man)
        95
        >>> count_non_zero(woman)
        97
        >>> count_non_zero(king)
        96
        >>> count_non_zero(queen)
        104

        >>> count_non_zero(man+woman)
        147
      
        >>> count_non_zero(man*woman)
        45
        >>> count_non_zero(woman*queen)
        15
        >>> count_non_zero(man*king)
        20
        >>> count_non_zero(king*queen)
        31

        >>> count_non_zero((man+woman)*king)
        24
        >>> count_non_zero(man*king)
        20
        >>> count_non_zero(woman*king)
        17

        >>> count_non_zero(man*woman*king*queen)
        4

        closest_dist('man:woman=king:queen')
        man is to woman as king is to ?queen?
          x+b-a           = king, queen, woman, mswati, vajiravudh, thoden, margrethe, thelred, gojong, mongkut, zvonimir, norodom, wenceslaus, urraca, monarch, hecuba, athelstan, eirik, princess, archelaus
          [x+b-a]         = king, queen, woman, princess, mswati, monarch, henry, vajiravudh, mongkut, eirik, gojong, margrethe, vii, thoden, zvonimir, prince, thelred, norodom, wenceslaus, urraca
          x+[b-a]         = king, woman, queen, henry, princess, monarch, mother, charles, daughter, son, prince, mswati, vii, vajiravudh, mongkut, thelred, louis, eirik, gojong, margrethe
          [x-a]+b         = king, woman, man, queen, girl, mother, daughter, teenager, wife, son, women, her, princess, person, she, monarch, girlfriend, friend, men, husband
          [2x-a]+[2b-a]   = king, woman, queen, daughter, mother, monarch, princess, henry, girl, son, mongkut, mswati, wife, vajiravudh, margrethe, vii, prince, thelred, gojong, eirik
          x+[b-a]+b+[x-a] = king, woman, queen, mother, daughter, man, girl, son, princess, henry, wife, monarch, women, her, charles, she, teenager, prince, mongkut, mswati

        Problem (and insight?)
        >>> closest_dist('pound:england=franc:france')
        pound is to england as franc is to ?france? 
          x+b-a           = england, franc, wales, france, australia, switzerland, scotland, zealand, middlesex, manchester, essex, belgium, capello, portugal, spain, cameroon, lanka, indies, senegal, blanc
          [x+b-a]         = england, franc, wales, france, australia, switzerland, manchester, scotland, essex, middlesex, britain, lancashire, indies, zealand, capello, odi, portugal, cameroon, argentina, francs
          x+[b-a]         = franc, england, francs, wales, australia, peso, switzerland, france, middlesex, rupee, forint, dirham, manchester, odi, indies, dinar, ruble, essex, zealand, currency
          [x-a]+b         = england, franc, wales, australia, france, britain, scotland, switzerland, ireland, manchester, capello, spain, zealand, essex, country, middlesex, africa, lancashire, portugal, indies
          [2x-a]+[2b-a]   = england, franc, wales, australia, france, switzerland, francs, britain, scotland, middlesex, manchester, zealand, essex, indies, capello, odi, lancashire, portugal, spain, swiss
          x+[b-a]+b+[x-a] = franc, england, wales, australia, france, switzerland, francs, britain, scotland, manchester, middlesex, zealand, essex, indies, capello, lancashire, odi, spain, portugal, swiss
  
        Problem:
          The original 3 'base words' are often near the top of the chosen target list
            This happens in the linear (dense) case too
            The original words are deliberately removed from the 'choosable' outcomes
              Otherwise scores are much lower
            But this seems to indicate that the original words have a 'basin/catchment area) that is broader/deeper than the target
            
  
  In-Process  :: To be monetised?
    Palm tree detection

      Side-Note : Pixelwise Segmentation
      -------------------------------------------

      After lots of fumbling around, it's fairly clear that Theano libraries (Blocks, Lasagne, to name two)
      are not really looking at the pixelwise segmentation problem.  And most CNN work doesn't
      get there either : what we set out to do simply didn't match the components that others have created.

      End result : Either step through the image manually, or use a scatter-gun approach to pick 
      patches for training / testing.  It works pretty quickly, even though it doesn't have the appeal of
      whole-image-at-a-time training.


  In-Process  :: To be monetised?
    Identity card reading, from 'bad photos'


  Idea : 
    More labels for better beginning and ending characteristics ?
    Dual LSTM question is pretty-much resolved, though
    Unsupervised chunking?
    
    
  IDEA :
    Middle-out learning of grammar embedding, as above
      Stumbling block is testability of 'segmentation/structure' learned
  
  IDEA :
    Think about 'internal model' method of learning
      (Obviously) read the writing Chinese paper some more
      
  IDEA :
    Character-based NER (done by LSTM paper 2016-03)
  
  IDEA :
    Imagination for visual networks
  
  IDEA : 
    Hierarchical model derived a la InfoGAN
    WordNet is a human artifact
      Perhaps 'reasonable' hierarchies should have similar properties
        eg:  MNIST doesn't have 100 latent variables, nor(?) a binary decision tree
      Look into statistical properties of WordNet (branching factor, depth, etc)
    Are Hierarchical Dirichlet Processes relevant?
   
  IDEA : 
    Problem : Adversarial Image detection
    Solution :
      Train autoencoder to reconstruct final softmax layer 
      Use reconstruction error as indicator of whether the image is 'genuine'
      
  IDEA :
    Problem : Non-class detection
    Solution :  (Use, for simplicity, MNIST)
      Train InfoGAN with 10 independent latent variables on MNIST
      Instead of training supervised layer on just valid images, also add 'in-between' invalid images
      Can make nice clock-face style error diagram to illustrate 'no-man's-land' areas
      
  IDEA :
    Problem : Better sparse embedding
    Solution : (sketch)
      Each word should be an Indian buffet of concepts
      
  IDEA :
    Summarisation is lossy compression, thinking of something helpful for attention
    Use 'internal dialogue' as well as text to answer questions
      and post to internal dialogue for future reference
      
  IDEA :
    Split long texts into sub-units (~paragraphs, etc) for different topics (at different levels of sensitivity)
      summarise each chunk hierarchically
    Meaning occurs at different resolutions (~fractal)
      but also a Direichlet-like process per section
  
  IDEA : 
    Reinforcement learning : Summarise chuinks of history to reduce 'interval' to learn over
    
  
      
