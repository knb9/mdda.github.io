## See for helpful jekyll stuff : http://jekyllrb.com/docs/templates/
## https://github.com/dbtek/jekyll-bootstrap-3/

sudo yum install ruby-devel

## Instead : Add a Gemfile... and ::
gem install bundler
bundle install 

#if Ruby upgraded, simplest to ```rm -rf ~/.gem``` otherwise endless search through updates...
#And then : 
bundle update

## Painful route :
#bundle update 
## Problems : nokogiri, posix_spawn_ext

bundle exec jekyll serve --watch --unpublished



git config user.name "Martin Andrews"
git config user.email GitHub@mdda.net


### http://jekyllbootstrap3.tk/preview/#/theme/Dbyll
## rake theme:install git="https://github.com/jekyll-bs3/dbyll"


git add assets/themes/dbyll
git add _includes/themes/dbyll
git add _layouts/*

git commit -a -m "Gravatar MD5 included in _config.yml"

git commit -a -m "Put useful content in index.md - for homepage"


## Within the ai-blog directory, move yyyy-mm/title-text.md to yyyy-mm-01-title-text.md 
find . -name "*.md" | perl -n -e 'chomp(my $a=$_); (my $b=$a)=~s{\./}{}; $b =~s{\/}{-01-}; system(qq(cp $a $b));'

## Fix up the contents of the files
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\ntype: ai-blog}{\ncategory: AI}' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\nai-blog: \[index\]\n}{\n}' '{}' \;

find . -name "20*.md" -exec perl -0777 -i -p -e 's{\ntype: oss-blog}{\ncategory: OSS}' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\noss-blog: \[index\]\n}{\n}' '{}' \;

find . -name "20*.md" -exec perl -0777 -i -p -e 's{\n---\n.*?\={10,}\n}{\nlayout: post\nfrom_mdda_blog: true\n---\n{\% include JB/setup \%}\n\n}s' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\n---\n.*?\#\#.*?\n}{\nlayout: post\nfrom_mdda_blog: true\n---\n{\% include JB/setup \%}\n\n}s' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\ntagline:}{\nsubtitle:}' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\ntopics:}{\ntags:}' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\npublic: yes\n}{\n}' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\ndraft: true\n}{\npublished: false\n}i' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\nauthor: admin\n}{\n}' '{}' \;

## Fix up the syntax highlighting - for those without a language
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\n\`\`\`\s*(.*?)\n\`\`\`\n}{\n\{\% highlight bash \%\}\n$1\n\{\% endhighlight \%\}\n}gs' '{}' \;
## Fix up the syntax highlighting - for those with a language specified
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\n\`\`\`(.*?)\n(.*?)\n\`\`\`\n}{\n\{\% highlight $1 \%\}\n$2\n\{\% endhighlight \%\}\n}gs' '{}' \;

# Move to the appropriate directory
mv *.md ../../AI/
mv *.md ../../OSS/


### Fix up pages
# https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet

## Move over images
# Can't find any (??)

## Todo : Nicer TAG pages
http://stackoverflow.com/questions/1408824/an-easy-way-to-support-tags-in-a-jekyll-blog
http://christianspecht.de/tags/#reporting-services
http://docs.shopify.com/themes/liquid-documentation/filters/additional-filters

## Todo : GitHub Follow Button
http://ghbtns.com/
https://help.github.com/articles/repository-metadata-on-github-pages


Make MathJax work (??)

## Useful pages while writing :
  http://jekyllrb.com/docs/pages/
  https://github.com/dbtek/jekyll-bootstrap-3/
  https://github.com/jekyll-bootstrap-3/dbyll
  http://getbootstrap.com/css/#overview-container
  http://fortawesome.github.io/Font-Awesome/icons/

  http://getbootstrap.com/css/
  https://github.com/Shopify/liquid/wiki/Liquid-for-Designers


## Helpful : myfile=_posts/AI/ETC
## Helpful : touch --date '1 minute ago' ${myfile}
## Helpful : git commit --date="`stat -c %y ${myfile}`" ${myfile}



Want to do in-depth look at :
  2014/2015 NIPS:
    -- Looking through papers for something interesting to write about

  PCAnet (http://arxiv.org/abs/1404.3606)
    Definitely need to contact authors in SG! :: DEAD END - sigh!
    
  GoogleLeNet (http://arxiv.org/abs/1409.4842)
    Follow up :
      Lin-2014 "Network in Network" (http://arxiv.org/abs/1312.4400)
        Hmpf

  Rafai Papers :
    #1 Rafai 2011 "Contractive AutoEncoders - Explicit Invariance" (http://www.icml-2011.org/papers/455_icmlpaper.pdf)
      Need to look at code to judge changes required for non-normalized inputs (but range-bound outputs)
      Not clear whether sigmoid needs to be 'top-and-tailed' or could be one-sided (Relu)
      
    #2 Rafai 2011 "Manifold Tangent Classifier" (http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2011_1240.pdf)
      This appears less applicable to pure step-wise unsupervised learning, since it requires accumulation of various bundles of tangents, and then 'hind-sight'
      
    #3 Bengio 2012 "Better Mixing via Deep Representations" ()
      Interesting : This is much more of a hypothesis/discussion paper
      It's not clear that some of their hypotheses are alternatives (as appears to be suggested once or twice).  So the lack of proof for (a) doesn't imply evidence for (b), etc
      
    Homepage (includes links to Theano code) : 
      http://www-etud.iro.umontreal.ca/~rifaisal/
        https://github.com/lisa-lab/DeepLearningTutorials/blob/master/code/cA.py
        The cA.py code looks useful for the EEG task - having a go!
          Investigate ::
            Meaning of "Reconstruction Cost" (should it be negative? == No)
            And whether Jacobian should ever be NaN...  == No
            :: NEED TO NORMALIZE INPUTS to [0,1] ...
            :: Actually log(z) doesn't produce useful im(z) scaling-wise,
                 likely to be better to use polar( log(1+|z|), theta(z) ).{re, im}
    
  NER :     
    R. Grishman. Information extraction: Capabilities and challenges. Technical report, NYU Dept. CS, 2012. (http://cs.nyu.edu/grishman/tarragona.pdf)
  
    Useful from a literature review point of view - and goes through all the steps of IE nicely.
    But seems to run out of steam abruptly, rather than give nice "Further Work" ideas, for instance
    
  
  Google KV : Check refs :
    NER 
      [16] R. Grishman. Information extraction: Capabilities and challenges. Technical report, NYU Dept. CS, 2012. (http://cs.nyu.edu/grishman/tarragona.pdf)
      [18] READ B. Hachey, W. Radford, J. Nothman, M. Honnibal, and J. Curran. Evaluating entity linking with wikipedia. Artificial Intelligence, 194:130â€“150, 2013.  (http://benhachey.info/pubs/hachey-aij12-evaluating.pdf)
    Distance Training : 
      [29]  M. Mintz, S. Bills, R. Snow, and D. Jurafksy. Distant supervision for relation extraction without labeled data. In Prof. Conf. Recent Advances in NLP, 2009.  (http://web.stanford.edu/~jurafsky/mintz.pdf)
    Clustering of entities 
      [27] READ T. Mikolov, K. Chen, G. Corrado, and J. Dean.  Efficient estimation of word representations in vector space. In ICLR, 2013. (http://arxiv.org/abs/1301.3781)
      
  Socher-Manning video presentation again :: 
    http://techtalks.tv/talks/deep-learning-for-nlp-without-magic-part-1/58414/
    http://nlp.stanford.edu/courses/NAACL2013/NAACL2013-Socher-Manning-DeepLearning.pdf  (downloaded)
    http://www.socher.org/index.php/Main/ParsingWithCompositionalVectorGrammars
    NN for NER :: http://nlp.stanford.edu/~socherr/pa4_ner.pdf
    
  Dropout
    Hinton, Srivastava, Krizhevsky, Sutskever &amp; Salakhutdinov - 2012 (http://arxiv.org/abs/1207.0580)
      Minor point : TIMIT shares some features with EEG task
    Several things being tested, besides drop-out :
      Weight constraints (/rebalancing) instead of penalties
      Dropping 20% of input pixels for MNIST (no other 'tricks' used)
    Similarity to random forest in terms of feature sampling  
    Excellent level of detail in describing model set-ups in the appendix
    Efficient batchwise dropout training using submatrices (2015)
      http://arxiv.org/pdf/1502.02478v1.pdf

  JPEG originators' keynote / paper / talk
    Contractive AutoEncoders : Rafai
      Standard RBMs (Hinton 2006) use transpose of Weights to project back, is this opposite of Harpur?
        http://stackoverflow.com/questions/20534237/deep-autoencoder-using-rbm
        http://www.cs.toronto.edu/~hinton/
        RBMs are different in that the feedback is biniarized (last step uses raw probability)
          But doesn't he do dropout later on to 'add back noise'?
        Isn't SEC 3.16 (p38 of PDF) the same as RBM learningTR (9) with contractive thingy added on?  (Hmmm- it's a suggestive link...)
        Isn't Harpur 4.15 (p63 of PDF) the same as ... (has minimised reconstruction built-in)?  (Hmmm- it's a suggestive link...)

  GloVe paper
    Follow up:
      Lebret and Collobert 2014 : HPCA?  :: Remi Lebret and Ronan Collobert. 2014. Word embeddings through Hellinger PCA. In EACL.
      
      Levy et al 2014 : PPMI (also alternative to cosine similarity) :: Omer Levy, Yoav Goldberg, and Israel Ramat-Gan. 2014. Linguistic regularities in sparse and explicit word representations. CoNLL-2014.


  Collobert deeper dive (http://ronan.collobert.com/pub/matos/2011_nlp_jmlr.pdf)
    This seems to be 'outsider' work, but state-of-the-art and as uncoloured by linguistic knowledge as possible
    Klein and Manning (2002) realistic hierarchical unsupervised grammar
    Has good appendix with layer descriptions in it
    Later paper : http://ronan.collobert.com/pub/matos/2011_parsing_aistats.pdf
      Code for 2008 version : https://github.com/turian/neural-language-model
    SOURCE :: http://ronan.collobert.com/senna/
  


Idea : Put up LibreOffice Python plugin notes

GPU stuff
  https://github.com/BIDData/BIDMach/wiki
  http://nlp.cs.berkeley.edu/pubs/Hall-BergKirkpatrick-Canny-Klein_2014_GPUParser_paper.pdf
  http://on-demand.gputechconf.com/gtc/2014/presentations/S4811-extreme-machine-learning-with-gpus.pdf
  Theano scan : http://nbviewer.ipython.org/gist/triangleinequality/1350873eebea33973e41
  
http://www.scipy.org/install.html
http://nbviewer.ipython.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb
http://ipython.org/ipython-doc/stable/notebook/notebook.html

Theano : 
  Theano implementation of SENNA NER network
    https://github.com/Fematich/nn_ner
  Deep Learning Tutorial : NLP/word-embedding
    http://deeplearning.net/tutorial/rnnslu.html
    
  (Multi-layer Hidden&ReLu + LogisticOutput) with ADAgrad 
    http://nbviewer.ipython.org/github/dawenl/deep_tagging/blob/master/code/deep_tagging.ipynb
    https://github.com/dawenl/deep_tagging

  nntools (now Lasagne?) (FF-NN focussed)
    https://github.com/benanne/Lasagne
   
  blocks : (More RNN-focussed)
    http://blocks.readthedocs.org/en/latest/
    
  http://nbviewer.ipython.org/github/lisa-lab/pylearn2/blob/master/pylearn2/scripts/tutorials/multilayer_perceptron/multilayer_perceptron.ipynb

Gelphi : Graph Data Visualization

CloudFoundry :: github.com/cf-platform-eng/cf-community-workshop
  Neo4j
  Redis
  Postgres MySQL
  Jenkins

https://help.github.com/articles/syncing-a-fork/
https://github.com/Kunena/Kunena-Forum/wiki/Create-a-new-branch-with-git-and-manage-branches

TODO :  Add reference to talk.js presentation on RedCatLabs.com (plus video link?)


NIPS ideas :
  Hinton et al : Grammar as a Foreign Language
    http://arxiv.org/abs/1412.7449v2  :: [v2] Sat, 28 Feb 2015 03:16:54 GMT (115kb)
    Google-2015_Grammar-as-Foreign-Langauge_1412.7449v2.pdf

  Collobert  (Possible equivalent performance-wise to Berkeley parser)
    Natural Language from (almost) Scratch :
      http://ronan.collobert.com/pub/matos/2011_nlp_jmlr.pdf
    Code (Non-Commercial)
      http://ronan.collobert.com/senna/
    Neural Net version (Cython) (documented, complete with NER examples)
      https://github.com/erickrf/nlpnet
    Neural Net version (Theano) (minimal)
      https://github.com/Fematich/nn_ner
      
  Stanford course 
    Socher : cs224d
      http://cs224d.stanford.edu/syllabus.html    

  Theano LSTM: 
    http://www.sphere-engineering.com/blog/quickanswers-io-a-new-algorithm.html
      https://github.com/keyua-cisco/theano-nlp
    Shawn Tan (NUS...)
      https://github.com/shawntan?tab=repositories
      https://blog.wtf.sg/about/
  
  spaCy: AGPL
    Same basic engine as the 500-lines blog post (which I've already trans-coded)
    Docs
      http://honnibal.github.io/spaCy/
    Code
      https://github.com/honnibal/spaCy
    NER : 
      13/04: Version 0.80 released. Includes named entity recognition, better sentence boundary detection, and many bug fixes.
      https://github.com/honnibal/spaCy/issues/62

  DEPENDENCY-BASED WORD EMBEDDINGS
    Use dependency-derived contexts to imply the word embeddings
      https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/
    
  Structural inference
    GTN
      Bottou-1997_GraphTransformerNetworks_gtn.pdf
    ParseTrees
      Collobert-2011_BuildingParseTrees_aistats.pdf
      Word embeddings initialised from previous work (helpful)
      Produced likelihoods of each tag at every position
      Viterbi for producing best sequence of symbols
      Contraints so that tree conditions satisfied
    Recursive NNs (Socher)
      http://cs224d.stanford.edu/lectures/CS224d-Lecture9.pdf
        Binarized trees sufficient for language parsing (up to a post-processing step)
        Also : PCFG hints for better tree 'tree pre-pruning' (but that requires pre-labelled training data too)
      https://www.youtube.com/watch?v=DJHvaGU9SW8
    
    Long Short-Term Memory Over Tree Structures  
      http://arxiv.org/abs/1503.04881     
    or Tai? in Socher's lab...  Submitted to ACML
 
  Check out 
    Socher : MV-RNN for Relationship Classification (looks good for Handshakes...)
    Socher : Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank 2013
  
-------------------  
Scheme for : "Middle out" Learning of Grammar Embeddings
  Learn word-vectors in the standard way, with left- and right-contexts being trained to 
    imply the middle 'idea' (which will initially just be the word, but will later be an 'idea')
    This will train the word vectors (if they are used, or the previous level's combiner and ancestors) 
      and the current context transformer
    
  Also, over the sentence, combine a pair together (using a combiner transformer), and
    do the same imputation for the middle 'idea' (this is the middle out piece)
    This will train the combiner transformer
    
  Looking at the whole sentence, pick the combiner error with the lowest value (i.e. most predicable idea in context)
    [ alternatively, for the null hypothesis, just pick a pair at random ]
    and replace that pair with the (true) result 
    And apply the procedure again, until left with one vector (the sentence 'idea')
      At which point, end

  
  Test word vectors with diagram of embedding learned (null should not care(?))
  
  See whether grammar has been learned by training a simple POS classifier on 'idea' vectors vs null
    But POS only actually labels word 'ideas'.  
    And these are trained to be insensitive to the current context (i.e. always the same, independent of the current sentence)
    So, in what sense would grammar get adsorbed into the word vectors?
      The tree-ification would have to provide additional information over-and-above the linear structure / local context
      How could that piece be fed as an extra feature alongside the word vectors?
        Perhaps as a simple indicator as to whether the word becomes a left or a right leaf?
          But a lot of that associativity is built into the linear structure already
            Information would have to indicate higher-order associations
            A lot of work would have to be done before this glimmer of light could be unveiled...
  
  Test to see whether parse trees make sense vs null too

  
  ... Time to figure out what 'simple scheme' for the context transformer
  ... (and then the combiner transformer too)
  
  
  Simplest route is to play with Collobert models

    Neural Net version (Cython) (documented, complete with NER examples)
      https://github.com/erickrf/nlpnet
    Neural Net version (Theano) (minimal)
      https://github.com/Fematich/nn_ner

    http://matpalm.com/blog/2015/03/28/theano_word_embeddings/

  Add curruculum idea too
  And cite Klein Manning 2001
  
  What would it mean to train LSTM on sentences 'unsupervised'?
-------------------

  
  
Alternative : 

  DONE 
    Create Entity extraction corpuses from output of :
      Stanford NER
      Berkeley NER

    Then train LSTM network on those corpuses

    Requires out-of-sample validation set 
      and common test sets
    
    Corpus to use : CoNLL-2003, which has NER training, test and 'raw' sets 
      (as well as 1Gb of additional corpus data)
    
    See whether network can (a) replicate and (b) supercede 
    
    Secondary goal : 
      Reduce need to rely on anti-commercial licensed software
    
    Other avenues: 
      Curriculum training based on (say) sentence length
        Or degree of agreement from corpus builders

      Additional NER methods, so that can do voting, and ensembling
    
  In-Process
    Binary re-representation of word embeddings
    
  In-Process
    Palm tree detection
    
  IDEA
    Middle-out learning of grammar embedding, as above
      Stumbling block is testability of 'segmentation/structure' learned
  
  IDEA
    Think about 'internal model' method of learning
      (Obviously) read the writing Chinese paper some more
      
  IDEA
    Character-based NER
  
  IDEA 
    Imagination for visual networks
    

Venue : 
  PyData
  
  FOSSASIA
    Topic :  Deep Learning 
      Either : 
        Talk of 20 minutes 
          simple to demo (own laptop, etc)
        Workshop of 1 or 2 hours  (very tempting, except for the planning/time committment)
          Michal loves the idea 
            and also thinks that DeepDream is a really interesting topic
              suggestion : VirtualBox images are pretty portable
                if we only need iPython with all the imports, don't need to worry about editor, etc
                could also do it without installing X - and running the browser session in the host machine...
                unfortunately VM solution probably doesn't allow for GPU anyway : 
                  https://github.com/BVLC/caffe/issues/2749
                successful installers could also open up their 'server' to access via WiFi, without risk
                  https://nsrc.org/workshops/2014/btnog/raw-attachment/wiki/Track2Agenda/ex-virtualbox-portforward-ssh.htm
                  https://www.reddit.com/r/IPython/comments/3gw3e5/how_to_access_to_the_ipython_server_from_a/
                http://blog.ouseful.info/2014/12/12/seven-ways-of-running-ipython-notebooks/
                  http://blog.ouseful.info/2015/05/18/kiteflying-around-containers-a-better-alternative-to-course-vms/
                http://lambdaops.com/ipythonjupyter-tmpnb-debuts/                
        
          ConvNets:
            Pros 
              More materials available
              Different angles : Art, Commerce
              Can do some simple 'real' training (MNIST)
              Can also use pre-built model (ImageNet / Inception-3)
            Cons
              'Simple stuff'
              Overlap with potential customers?
              Google DeepDreaming may not be ready : CHECK THIS
            Ideas for progression :
              Stanford Seminar - Oriol Vinyals of Google
                https://youtu.be/UAq961jQjYg

          RNNs / NLP:
            Pros 
              More similar to current work
            Cons
              Difficult to set up satisfying motivation / experiments

          Reinforcement Learning:
            Pros 
              Go=Sexy
            Cons
              Not so clearly 'deep learning'
              Very Difficult to set up satisfying motivation / experiments
              Very Difficult to set up satisfying motivation / experiments
              Someone else might do it...
                
          What environment to use?  
            VPS?  expense...
            
            docker images?  
              GPU would be tricky to use, and there's no OS-independence
                http://stackoverflow.com/questions/25185405/using-gpu-from-a-docker-container
                https://github.com/NVIDIA/nvidia-docker
              Can run on windows (it essentially builds on top of a Virtualized Linux environment)
                https://docs.docker.com/engine/installation/windows/
            
            USB Live environments?  
            
            iPython (somehow)?
              https://github.com/donnemartin/data-science-ipython-notebooks
              https://github.com/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/deep-dream/dream.ipynb
                http://googleresearch.blogspot.sg/2015/07/deepdream-code-example-for-visualizing.html
              http://www.pyimagesearch.com/2015/07/06/bat-country-an-extendible-lightweight-python-package-for-deep-dreaming-with-caffe-and-convolutional-neural-networks/#show_and_tell  
                https://github.com/jrosebr1/bat-country/blob/master/demo.py
              https://github.com/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/5_word2vec.ipynb

            Using Caffe models in Lasagne
              https://github.com/Lasagne/Lasagne/issues/157

            TensorFlow + iPython + VM
              https://github.com/gavinln/tensorflow-ipy
              http://stackoverflow.com/questions/33784214/how-to-test-tensorflow-cifar10-cnn-tutorial-model?rq=1


          Model Zoo ideas:
            https://groups.google.com/a/tensorflow.org/forum/m/#!topic/discuss/BKOiAgKDxqM
            But decent ImageNet model is ~60Mb => need ThumbDrives
          Inception 3 :
            https://www.tensorflow.org/versions/master/tutorials/image_recognition/index.html
          Inception 3 / Deep Dream = Coming soon (bottom of page) :
            https://www.tensorflow.org/versions/master/tutorials/index.html
          Caffe :
            http://caffe.berkeleyvision.org/gathered/examples/imagenet.html
          Lasagne :
            https://github.com/Lasagne/Recipes/tree/master/modelzoo

              
          Thumb drive ideas:
            http://www.aliexpress.com/item/Usb-flash-drive-4gb-8gb-16gb-32gb-cat-s-claw-usb-pendrive-gift-u-disk-memory/32424298881.html?spm=2114.01010208.3.319.T9dcav&ws_ab_test=searchweb201556_1,searchweb201644_1_505_506_503_504_502_10001_9705_10002_10016_10005_10006_10003_10004_62,searchweb201560_1,searchweb1451318400_6151,searchweb1451318411_6452&btsid=657fe2dd-426e-4442-9694-674c828df7b0
            http://www.aliexpress.com/item/Wholesale-and-custom-Cat-model-of-metal-gift-art-creative-personality-key-usb-flash-drive-16gb/32323841251.html?spm=2114.01010208.3.10.YqupqE&ws_ab_test=searchweb201556_1,searchweb201644_1_505_506_503_504_502_10001_9705_10002_10016_10005_10006_10003_10004_62,searchweb201560_1,searchweb1451318400_6151,searchweb1451318411_6452&btsid=5d059c6f-8cb2-440f-b9d3-87d3b94f4f3b
              
          Other peoples'
            http://www.slideshare.net/roelofp/deep-learning-as-a-catdog-detector
            
          JS in the browser :
            Pros
              CPU-only
              Single-threaded
              Avoids thumb-drive distribution
            Cons
              Not Python
              No GPU or multi-core - though this is (basically) same as with VM or docker
              Not iPython 
            Code
              https://github.com/karpathy/convnetjs
              https://github.com/karpathy/reinforcejs
                There is a new paper in regards to deep reinforcements learning in continous spaces by deepmind. 
                Continuous control with deep reinforcements learning. Is there plans to add this in code form. Many thanks Andrew.

              
      Foundation : Python (or iPython)
        TensorFlow?  (would require more work)
        Theano + Lasagne (standing on head)
      
      Thumbs up from organiser on DeepLearning idea
        Not too worried about track selection, etc - all a work-in-progress
    
  
  PyCon
  
  SGDS Lightning Talk
  
