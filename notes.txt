## See for helpful jekyll stuff : http://jekyllrb.com/docs/templates/
## https://github.com/dbtek/jekyll-bootstrap-3/

sudo yum install ruby-devel

## Instead : Add a Gemfile... and ::
gem install bundler
bundle install 

#if Ruby upgraded, simplest to ```rm -rf ~/.gem``` otherwise endless search through updates...

## Painful route :
#bundle update 
## Problems : nokogiri, posix_spawn_ext

bundle exec jekyll serve --watch --unpublished


### http://jekyllbootstrap3.tk/preview/#/theme/Dbyll
## rake theme:install git="https://github.com/jekyll-bs3/dbyll"


git add assets/themes/dbyll
git add _includes/themes/dbyll
git add _layouts/*

git commit -a -m "Gravatar MD5 included in _config.yml"

git commit -a -m "Put useful content in index.md - for homepage"


## Within the ai-blog directory, move yyyy-mm/title-text.md to yyyy-mm-01-title-text.md 
find . -name "*.md" | perl -n -e 'chomp(my $a=$_); (my $b=$a)=~s{\./}{}; $b =~s{\/}{-01-}; system(qq(cp $a $b));'

## Fix up the contents of the files
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\ntype: ai-blog}{\ncategory: AI}' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\nai-blog: \[index\]\n}{\n}' '{}' \;

find . -name "20*.md" -exec perl -0777 -i -p -e 's{\ntype: oss-blog}{\ncategory: OSS}' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\noss-blog: \[index\]\n}{\n}' '{}' \;

find . -name "20*.md" -exec perl -0777 -i -p -e 's{\n---\n.*?\={10,}\n}{\nlayout: post\nfrom_mdda_blog: true\n---\n{\% include JB/setup \%}\n\n}s' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\n---\n.*?\#\#.*?\n}{\nlayout: post\nfrom_mdda_blog: true\n---\n{\% include JB/setup \%}\n\n}s' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\ntagline:}{\nsubtitle:}' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\ntopics:}{\ntags:}' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\npublic: yes\n}{\n}' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\ndraft: true\n}{\npublished: false\n}i' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\nauthor: admin\n}{\n}' '{}' \;

## Fix up the syntax highlighting - for those without a language
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\n\`\`\`\s*(.*?)\n\`\`\`\n}{\n\{\% highlight bash \%\}\n$1\n\{\% endhighlight \%\}\n}gs' '{}' \;
## Fix up the syntax highlighting - for those with a language specified
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\n\`\`\`(.*?)\n(.*?)\n\`\`\`\n}{\n\{\% highlight $1 \%\}\n$2\n\{\% endhighlight \%\}\n}gs' '{}' \;

# Move to the appropriate directory
mv *.md ../../AI/
mv *.md ../../OSS/


### Fix up pages
# https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet

## Move over images
# Can't find any (??)

## Todo : Nicer TAG pages
http://stackoverflow.com/questions/1408824/an-easy-way-to-support-tags-in-a-jekyll-blog
http://christianspecht.de/tags/#reporting-services
http://docs.shopify.com/themes/liquid-documentation/filters/additional-filters

## Todo : GitHub Follow Button
http://ghbtns.com/
https://help.github.com/articles/repository-metadata-on-github-pages


Make MathJax work (??)

## Useful pages while writing :
  http://jekyllrb.com/docs/pages/
  https://github.com/dbtek/jekyll-bootstrap-3/
  https://github.com/jekyll-bootstrap-3/dbyll
  http://getbootstrap.com/css/#overview-container
  http://fortawesome.github.io/Font-Awesome/icons/

  http://getbootstrap.com/css/
  https://github.com/Shopify/liquid/wiki/Liquid-for-Designers


Want to do in-depth look at :
  2014 NIPS:
    -- Looking through papers for something interesting to write about

  PCAnet (http://arxiv.org/abs/1404.3606)
    Definitely need to contact authors in SG! :: DEAD END - sigh!
    
  GoogleLeNet (http://arxiv.org/abs/1409.4842)
    Follow up :
      Lin-2014 "Network in Network" (http://arxiv.org/abs/1312.4400)
        Hmpf

  Rafai Papers :
    #1 Rafai 2011 "Contractive AutoEncoders - Explicit Invariance" (http://www.icml-2011.org/papers/455_icmlpaper.pdf)
      Need to look at code to judge changes required for non-normalized inputs (but range-bound outputs)
      Not clear whether sigmoid needs to be 'top-and-tailed' or could be one-sided (Relu)
      
    #2 Rafai 2011 "Manifold Tangent Classifier" (http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2011_1240.pdf)
      This appears less applicable to pure step-wise unsupervised learning, since it requires accumulation of various bundles of tangents, and then 'hind-sight'
      
    #3 Bengio 2012 "Better Mixing via Deep Representations" ()
      Interesting : This is much more of a hypothesis/discussion paper
      It's not clear that some of their hypotheses are alternatives (as appears to be suggested once or twice).  So the lack of proof for (a) doesn't imply evidence for (b), etc
      
    Homepage (includes links to Theano code) : 
      http://www-etud.iro.umontreal.ca/~rifaisal/
        https://github.com/lisa-lab/DeepLearningTutorials/blob/master/code/cA.py
        The cA.py code looks useful for the EEG task - having a go!
          Investigate ::
            Meaning of "Reconstruction Cost" (should it be negative? == No)
            And whether Jacobian should ever be NaN...  == No
            :: NEED TO NORMALIZE INPUTS to [0,1] ...
            :: Actually log(z) doesn't produce useful im(z) scaling-wise,
                 likely to be better to use polar( log(1+|z|), theta(z) ).{re, im}
    
  NER :     
    R. Grishman. Information extraction: Capabilities and challenges. Technical report, NYU Dept. CS, 2012. (http://cs.nyu.edu/grishman/tarragona.pdf)
  
    Useful from a literature review point of view - and goes through all the steps of IE nicely.
    But seems to run out of steam abruptly, rather than give nice "Further Work" ideas, for instance
    
  
  Google KV : Check refs :
    NER 
      [16] R. Grishman. Information extraction: Capabilities and challenges. Technical report, NYU Dept. CS, 2012. (http://cs.nyu.edu/grishman/tarragona.pdf)
      [18] READ B. Hachey, W. Radford, J. Nothman, M. Honnibal, and J. Curran. Evaluating entity linking with wikipedia. Artificial Intelligence, 194:130â€“150, 2013.  (http://benhachey.info/pubs/hachey-aij12-evaluating.pdf)
    Distance Training : 
      [29]  M. Mintz, S. Bills, R. Snow, and D. Jurafksy. Distant supervision for relation extraction without labeled data. In Prof. Conf. Recent Advances in NLP, 2009.  (http://web.stanford.edu/~jurafsky/mintz.pdf)
    Clustering of entities 
      [27] READ T. Mikolov, K. Chen, G. Corrado, and J. Dean.  Efficient estimation of word representations in vector space. In ICLR, 2013. (http://arxiv.org/abs/1301.3781)
      
  Socher-Manning video presentation again :: 
    http://techtalks.tv/talks/deep-learning-for-nlp-without-magic-part-1/58414/
    http://nlp.stanford.edu/courses/NAACL2013/NAACL2013-Socher-Manning-DeepLearning.pdf  (downloaded)
    http://www.socher.org/index.php/Main/ParsingWithCompositionalVectorGrammars
    NN for NER :: http://nlp.stanford.edu/~socherr/pa4_ner.pdf
    
  Dropout
    Hinton, Srivastava, Krizhevsky, Sutskever &amp; Salakhutdinov - 2012 (http://arxiv.org/abs/1207.0580)
      Minor point : TIMIT shares some features with EEG task
    Several things being tested, besides drop-out :
      Weight constraints (/rebalancing) instead of penalties
      Dropping 20% of input pixels for MNIST (no other 'tricks' used)
    Similarity to random forest in terms of feature sampling  
    Excellent level of detail in describing model set-ups in the appendix
    Efficient batchwise dropout training using submatrices (2015)
      http://arxiv.org/pdf/1502.02478v1.pdf

  JPEG originators' keynote / paper / talk
    Contractive AutoEncoders : Rafai
      Standard RBMs (Hinton 2006) use transpose of Weights to project back, is this opposite of Harpur?
        http://stackoverflow.com/questions/20534237/deep-autoencoder-using-rbm
        http://www.cs.toronto.edu/~hinton/
        RBMs are different in that the feedback is biniarized (last step uses raw probability)
          But doesn't he do dropout later on to 'add back noise'?
        Isn't SEC 3.16 (p38 of PDF) the same as RBM learningTR (9) with contractive thingy added on?  (Hmmm- it's a suggestive link...)
        Isn't Harpur 4.15 (p63 of PDF) the same as ... (has minimised reconstruction built-in)?  (Hmmm- it's a suggestive link...)

  GloVe paper
    Follow up:
      Lebret and Collobert 2014 : HPCA?  :: Remi Lebret and Ronan Collobert. 2014. Word embeddings through Hellinger PCA. In EACL.
      
      Levy et al 2014 : PPMI (also alternative to cosine similarity) :: Omer Levy, Yoav Goldberg, and Israel Ramat-Gan. 2014. Linguistic regularities in sparse and explicit word representations. CoNLL-2014.


  Collobert deeper dive (http://ronan.collobert.com/pub/matos/2011_nlp_jmlr.pdf)
    This seems to be 'outsider' work, but state-of-the-art and as uncoloured by linguistic knowledge as possible
    Klein and Manning (2002) realistic hierarchical unsupervised grammar
    Has good appendix with layer descriptions in it
    Later paper : http://ronan.collobert.com/pub/matos/2011_parsing_aistats.pdf
      Code for 2008 version : https://github.com/turian/neural-language-model
    SOURCE :: http://ronan.collobert.com/senna/
  


Idea : Put up LibreOffice Python plugin notes

GPU stuff
  https://github.com/BIDData/BIDMach/wiki
  http://nlp.cs.berkeley.edu/pubs/Hall-BergKirkpatrick-Canny-Klein_2014_GPUParser_paper.pdf
  http://on-demand.gputechconf.com/gtc/2014/presentations/S4811-extreme-machine-learning-with-gpus.pdf
  Theano scan : http://nbviewer.ipython.org/gist/triangleinequality/1350873eebea33973e41
  
http://www.scipy.org/install.html
http://nbviewer.ipython.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb
http://ipython.org/ipython-doc/stable/notebook/notebook.html

Theano : 
  Theano implementation of SENNA NER network
    https://github.com/Fematich/nn_ner
  Deep Learning Tutorial : NLP/word-embedding
    http://deeplearning.net/tutorial/rnnslu.html
    
  (Multi-layer Hidden&ReLu + LogisticOutput) with ADAgrad 
	http://nbviewer.ipython.org/github/dawenl/deep_tagging/blob/master/code/deep_tagging.ipynb
	https://github.com/dawenl/deep_tagging
	
  nntools (now Lasagne?) (FF-NN focussed)
    https://github.com/benanne/Lasagne
   
  blocks : (More RNN-focussed)
    http://blocks.readthedocs.org/en/latest/
    
  http://nbviewer.ipython.org/github/lisa-lab/pylearn2/blob/master/pylearn2/scripts/tutorials/multilayer_perceptron/multilayer_perceptron.ipynb

Gelphi : Graph Data Visualization

CloudFoundry :: github.com/cf-platform-eng/cf-community-workshop
  Neo4j
  Redis
  Postgres MySQL
  Jenkins

https://help.github.com/articles/syncing-a-fork/
https://github.com/Kunena/Kunena-Forum/wiki/Create-a-new-branch-with-git-and-manage-branches

DONE : 
## location ~ ^/2014-06-15_SentenceDependencies-D3.js/ {
## location ~ ^/2014-08-25_OCR-v0.01/ {
## location ~ ^/2014-06-05_Reverse-GoL/ {
## location ~ ^/2014-08-27_shader-school/ {                 
## location ~ ^/2014-10-14_Reverse-GoL/ {
## location ~ ^/2014-12-10_ParsingEnglish.scala/ {
## location ~ ^/2014-12-18_DeepLearning.js/ {
## location ~ ^/2015-01-15_Presentation-PyDataSG/ {

TODO :  Add reference to all of these on RedCatLabs.com



Ideas for AGI conf
  Needs to be done and written-up by 15-March
    Basics implemented this weekend
      i.e. decide on project by Saturday night...
    Project finished next weekend
    Submitted on time
    
  Dropout as random ensembles
    Structured vs unstructured dropout
      Risky, since not clear what the aim is
      
  Teacher / Student networks
    Slightly sexier topic, since so current
    Reframe as being auto-improver
      Parasitic side network 
    Understand claims about Curriculum Learning in Teacher/Student paper
    Does teaching student aim at :
      improving generalization
      higher speed to learn
      greater efficiency of representation (valid biological goal)
    Strange that student networks can be so much smaller and as effective

  Why did earlier NNs fail?  
    Repeat old experiments?
      Earliest MNIST experiments ?
        But error rate was pretty low then anyway
  
  Sparse representations
    This seems more effective - a special lesson from nature
    Perhaps real-world situations are particularly amenable to sparse representations
      Multiple labels rather than single folders
      ~ Photos are compressible via (designed) JPEG
  
  Normalization  
    Whitening layers
    Weight initialization
    ReLu with LHS hard zero (and zero gradient)
    Vanishing gradient problem should be less material with floating point numbers
      But does seem to be an issue worth publishing about

  ReLu+ vs sigmoidal
    Is it effective just because of lower computational cost?
  
  Word Embedding
  
  Genetic Algorithms for training NNs
    Unclear whether it would work
  
  List of 100 problems
