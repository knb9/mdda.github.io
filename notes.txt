## See for helpful jekyll stuff : http://jekyllrb.com/docs/templates/
## https://github.com/dbtek/jekyll-bootstrap-3/

sudo dnf install ruby-devel

## Instead : Add a Gemfile... and ::
gem install bundler
bundle install 

#if Ruby upgraded, simplest to ```rm -rf ~/.gem``` otherwise endless search through updates...
#And then : 
bundle update

## Painful route :
#bundle update 
## Problems : nokogiri, posix_spawn_ext

bundle exec jekyll serve --watch --unpublished --future



git config user.name "Martin Andrews"
git config user.email GitHub@mdda.net


### http://jekyllbootstrap3.tk/preview/#/theme/Dbyll
## rake theme:install git="https://github.com/jekyll-bs3/dbyll"


git add assets/themes/dbyll
git add _includes/themes/dbyll
git add _layouts/*

git commit -a -m "Gravatar MD5 included in _config.yml"

git commit -a -m "Put useful content in index.md - for homepage"


## Within the ai-blog directory, move yyyy-mm/title-text.md to yyyy-mm-01-title-text.md 
find . -name "*.md" | perl -n -e 'chomp(my $a=$_); (my $b=$a)=~s{\./}{}; $b =~s{\/}{-01-}; system(qq(cp $a $b));'

## Fix up the contents of the files
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\ntype: ai-blog}{\ncategory: AI}' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\nai-blog: \[index\]\n}{\n}' '{}' \;

find . -name "20*.md" -exec perl -0777 -i -p -e 's{\ntype: oss-blog}{\ncategory: OSS}' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\noss-blog: \[index\]\n}{\n}' '{}' \;

find . -name "20*.md" -exec perl -0777 -i -p -e 's{\n---\n.*?\={10,}\n}{\nlayout: post\nfrom_mdda_blog: true\n---\n{\% include JB/setup \%}\n\n}s' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\n---\n.*?\#\#.*?\n}{\nlayout: post\nfrom_mdda_blog: true\n---\n{\% include JB/setup \%}\n\n}s' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\ntagline:}{\nsubtitle:}' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\ntopics:}{\ntags:}' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\npublic: yes\n}{\n}' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\ndraft: true\n}{\npublished: false\n}i' '{}' \;
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\nauthor: admin\n}{\n}' '{}' \;

## Fix up the syntax highlighting - for those without a language
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\n\`\`\`\s*(.*?)\n\`\`\`\n}{\n\{\% highlight bash \%\}\n$1\n\{\% endhighlight \%\}\n}gs' '{}' \;
## Fix up the syntax highlighting - for those with a language specified
find . -name "20*.md" -exec perl -0777 -i -p -e 's{\n\`\`\`(.*?)\n(.*?)\n\`\`\`\n}{\n\{\% highlight $1 \%\}\n$2\n\{\% endhighlight \%\}\n}gs' '{}' \;

# Move to the appropriate directory
mv *.md ../../AI/
mv *.md ../../OSS/


### Fix up pages
# https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet

## Move over images
# Can't find any (??)

## Todo : Nicer TAG pages
http://stackoverflow.com/questions/1408824/an-easy-way-to-support-tags-in-a-jekyll-blog
http://christianspecht.de/tags/#reporting-services
http://docs.shopify.com/themes/liquid-documentation/filters/additional-filters

## Todo : GitHub Follow Button
http://ghbtns.com/
https://help.github.com/articles/repository-metadata-on-github-pages


Make MathJax work (??)

## Useful pages while writing :
  http://jekyllrb.com/docs/pages/
  https://github.com/dbtek/jekyll-bootstrap-3/
  https://github.com/jekyll-bootstrap-3/dbyll
  http://getbootstrap.com/css/#overview-container
  http://fortawesome.github.io/Font-Awesome/icons/

  http://getbootstrap.com/css/
  https://github.com/Shopify/liquid/wiki/Liquid-for-Designers


## Helpful : myfile=_posts/AI/ETC
## Helpful : touch --date '1 minute ago' ${myfile}
## Helpful : git commit --date="`stat -c %y ${myfile}`" ${myfile} -m "Last minute update"



Want to do in-depth look at :
  2014/2015 NIPS:
    -- Looking through papers for something interesting to write about

  PCAnet (http://arxiv.org/abs/1404.3606)
    Definitely need to contact authors in SG! :: DEAD END - sigh!
    
  GoogleLeNet (http://arxiv.org/abs/1409.4842)
    Follow up :
      Lin-2014 "Network in Network" (http://arxiv.org/abs/1312.4400)
        Hmpf

  Rafai Papers :
    #1 Rafai 2011 "Contractive AutoEncoders - Explicit Invariance" (http://www.icml-2011.org/papers/455_icmlpaper.pdf)
      Need to look at code to judge changes required for non-normalized inputs (but range-bound outputs)
      Not clear whether sigmoid needs to be 'top-and-tailed' or could be one-sided (Relu)
      
    #2 Rafai 2011 "Manifold Tangent Classifier" (http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2011_1240.pdf)
      This appears less applicable to pure step-wise unsupervised learning, since it requires accumulation of various bundles of tangents, and then 'hind-sight'
      
    #3 Bengio 2012 "Better Mixing via Deep Representations" ()
      Interesting : This is much more of a hypothesis/discussion paper
      It's not clear that some of their hypotheses are alternatives (as appears to be suggested once or twice).  So the lack of proof for (a) doesn't imply evidence for (b), etc
      
    Homepage (includes links to Theano code) : 
      http://www-etud.iro.umontreal.ca/~rifaisal/
        https://github.com/lisa-lab/DeepLearningTutorials/blob/master/code/cA.py
        The cA.py code looks useful for the EEG task - having a go!
          Investigate ::
            Meaning of "Reconstruction Cost" (should it be negative? == No)
            And whether Jacobian should ever be NaN...  == No
            :: NEED TO NORMALIZE INPUTS to [0,1] ...
            :: Actually log(z) doesn't produce useful im(z) scaling-wise,
                 likely to be better to use polar( log(1+|z|), theta(z) ).{re, im}
    
  NER :     
    R. Grishman. Information extraction: Capabilities and challenges. Technical report, NYU Dept. CS, 2012. (http://cs.nyu.edu/grishman/tarragona.pdf)
  
    Useful from a literature review point of view - and goes through all the steps of IE nicely.
    But seems to run out of steam abruptly, rather than give nice "Further Work" ideas, for instance
    
  
  Google KV : Check refs :
    NER 
      [16] R. Grishman. Information extraction: Capabilities and challenges. Technical report, NYU Dept. CS, 2012. (http://cs.nyu.edu/grishman/tarragona.pdf)
      [18] READ B. Hachey, W. Radford, J. Nothman, M. Honnibal, and J. Curran. Evaluating entity linking with wikipedia. Artificial Intelligence, 194:130â€“150, 2013.  (http://benhachey.info/pubs/hachey-aij12-evaluating.pdf)
    Distance Training : 
      [29]  M. Mintz, S. Bills, R. Snow, and D. Jurafksy. Distant supervision for relation extraction without labeled data. In Prof. Conf. Recent Advances in NLP, 2009.  (http://web.stanford.edu/~jurafsky/mintz.pdf)
    Clustering of entities 
      [27] READ T. Mikolov, K. Chen, G. Corrado, and J. Dean.  Efficient estimation of word representations in vector space. In ICLR, 2013. (http://arxiv.org/abs/1301.3781)
      
  Socher-Manning video presentation again :: 
    http://techtalks.tv/talks/deep-learning-for-nlp-without-magic-part-1/58414/
    http://nlp.stanford.edu/courses/NAACL2013/NAACL2013-Socher-Manning-DeepLearning.pdf  (downloaded)
    http://www.socher.org/index.php/Main/ParsingWithCompositionalVectorGrammars
    NN for NER :: http://nlp.stanford.edu/~socherr/pa4_ner.pdf
    
  Dropout
    Hinton, Srivastava, Krizhevsky, Sutskever &amp; Salakhutdinov - 2012 (http://arxiv.org/abs/1207.0580)
      Minor point : TIMIT shares some features with EEG task
    Several things being tested, besides drop-out :
      Weight constraints (/rebalancing) instead of penalties
      Dropping 20% of input pixels for MNIST (no other 'tricks' used)
    Similarity to random forest in terms of feature sampling  
    Excellent level of detail in describing model set-ups in the appendix
    Efficient batchwise dropout training using submatrices (2015)
      http://arxiv.org/pdf/1502.02478v1.pdf

  JPEG originators' keynote / paper / talk
    Contractive AutoEncoders : Rafai
      Standard RBMs (Hinton 2006) use transpose of Weights to project back, is this opposite of Harpur?
        http://stackoverflow.com/questions/20534237/deep-autoencoder-using-rbm
        http://www.cs.toronto.edu/~hinton/
        RBMs are different in that the feedback is biniarized (last step uses raw probability)
          But doesn't he do dropout later on to 'add back noise'?
        Isn't SEC 3.16 (p38 of PDF) the same as RBM learningTR (9) with contractive thingy added on?  (Hmmm- it's a suggestive link...)
        Isn't Harpur 4.15 (p63 of PDF) the same as ... (has minimised reconstruction built-in)?  (Hmmm- it's a suggestive link...)

  GloVe paper
    Follow up:
      Lebret and Collobert 2014 : HPCA?  :: Remi Lebret and Ronan Collobert. 2014. Word embeddings through Hellinger PCA. In EACL.
      
      Levy et al 2014 : PPMI (also alternative to cosine similarity) :: Omer Levy, Yoav Goldberg, and Israel Ramat-Gan. 2014. Linguistic regularities in sparse and explicit word representations. CoNLL-2014.


  Collobert deeper dive (http://ronan.collobert.com/pub/matos/2011_nlp_jmlr.pdf)
    This seems to be 'outsider' work, but state-of-the-art and as uncoloured by linguistic knowledge as possible
    Klein and Manning (2002) realistic hierarchical unsupervised grammar
    Has good appendix with layer descriptions in it
    Later paper : http://ronan.collobert.com/pub/matos/2011_parsing_aistats.pdf
      Code for 2008 version : https://github.com/turian/neural-language-model
    SOURCE :: http://ronan.collobert.com/senna/
  


Idea : Put up LibreOffice Python plugin notes

GPU stuff
  https://github.com/BIDData/BIDMach/wiki
  http://nlp.cs.berkeley.edu/pubs/Hall-BergKirkpatrick-Canny-Klein_2014_GPUParser_paper.pdf
  http://on-demand.gputechconf.com/gtc/2014/presentations/S4811-extreme-machine-learning-with-gpus.pdf
  Theano scan : http://nbviewer.ipython.org/gist/triangleinequality/1350873eebea33973e41
  
http://www.scipy.org/install.html
http://nbviewer.ipython.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb
http://ipython.org/ipython-doc/stable/notebook/notebook.html

Theano : 
  Theano implementation of SENNA NER network
    https://github.com/Fematich/nn_ner
  Deep Learning Tutorial : NLP/word-embedding
    http://deeplearning.net/tutorial/rnnslu.html
    
  (Multi-layer Hidden&ReLu + LogisticOutput) with ADAgrad 
    http://nbviewer.ipython.org/github/dawenl/deep_tagging/blob/master/code/deep_tagging.ipynb
    https://github.com/dawenl/deep_tagging

  nntools (now Lasagne?) (FF-NN focussed)
    https://github.com/benanne/Lasagne
   
  blocks : (More RNN-focussed)
    http://blocks.readthedocs.org/en/latest/
    
  http://nbviewer.ipython.org/github/lisa-lab/pylearn2/blob/master/pylearn2/scripts/tutorials/multilayer_perceptron/multilayer_perceptron.ipynb

Gelphi : Graph Data Visualization

CloudFoundry :: github.com/cf-platform-eng/cf-community-workshop
  Neo4j
  Redis
  Postgres MySQL
  Jenkins

https://help.github.com/articles/syncing-a-fork/
https://github.com/Kunena/Kunena-Forum/wiki/Create-a-new-branch-with-git-and-manage-branches

TODO :  Add reference to talk.js presentation on RedCatLabs.com (plus video link?)


NIPS ideas :
  Hinton et al : Grammar as a Foreign Language
    http://arxiv.org/abs/1412.7449v2  :: [v2] Sat, 28 Feb 2015 03:16:54 GMT (115kb)
    Google-2015_Grammar-as-Foreign-Langauge_1412.7449v2.pdf

  Collobert  (Possible equivalent performance-wise to Berkeley parser)
    Natural Language from (almost) Scratch :
      http://ronan.collobert.com/pub/matos/2011_nlp_jmlr.pdf
    Code (Non-Commercial)
      http://ronan.collobert.com/senna/
    Neural Net version (Cython) (documented, complete with NER examples)
      https://github.com/erickrf/nlpnet
    Neural Net version (Theano) (minimal)
      https://github.com/Fematich/nn_ner
      
  Stanford course 
    Socher : cs224d
      http://cs224d.stanford.edu/syllabus.html    

  Theano LSTM: 
    http://www.sphere-engineering.com/blog/quickanswers-io-a-new-algorithm.html
      https://github.com/keyua-cisco/theano-nlp
    Shawn Tan (NUS...)
      https://github.com/shawntan?tab=repositories
      https://blog.wtf.sg/about/
  
  spaCy: --AGPL-- - Now MIT
    Same basic engine as the 500-lines blog post (which I've already trans-coded)
    Docs
      http://honnibal.github.io/spaCy/
    Code
      https://github.com/honnibal/spaCy
    NER : 
      13/04: Version 0.80 released. Includes named entity recognition, better sentence boundary detection, and many bug fixes.
      https://github.com/honnibal/spaCy/issues/62

  DEPENDENCY-BASED WORD EMBEDDINGS
    Use dependency-derived contexts to imply the word embeddings
      https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/
    
  Structural inference
    GTN
      Bottou-1997_GraphTransformerNetworks_gtn.pdf
    ParseTrees
      Collobert-2011_BuildingParseTrees_aistats.pdf
      Word embeddings initialised from previous work (helpful)
      Produced likelihoods of each tag at every position
      Viterbi for producing best sequence of symbols
      Contraints so that tree conditions satisfied
    Recursive NNs (Socher)
      http://cs224d.stanford.edu/lectures/CS224d-Lecture9.pdf
        Binarized trees sufficient for language parsing (up to a post-processing step)
        Also : PCFG hints for better tree 'tree pre-pruning' (but that requires pre-labelled training data too)
      https://www.youtube.com/watch?v=DJHvaGU9SW8
    
    Long Short-Term Memory Over Tree Structures  
      http://arxiv.org/abs/1503.04881     
    or Tai? in Socher's lab...  Submitted to ACML
 
    
    
 
 
  Check out 
    Socher : MV-RNN for Relationship Classification (looks good for Handshakes...)
    Socher : Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank 2013
  
-------------------  
Scheme for : "Middle out" Learning of Grammar Embeddings
  Learn word-vectors in the standard way, with left- and right-contexts being trained to 
    imply the middle 'idea' (which will initially just be the word, but will later be an 'idea')
    This will train the word vectors (if they are used, or the previous level's combiner and ancestors) 
      and the current context transformer
    
  Also, over the sentence, combine a pair together (using a combiner transformer), and
    do the same imputation for the middle 'idea' (this is the middle out piece)
    This will train the combiner transformer
    
  Looking at the whole sentence, pick the combiner error with the lowest value (i.e. most predicable idea in context)
    [ alternatively, for the null hypothesis, just pick a pair at random ]
    and replace that pair with the (true) result 
    And apply the procedure again, until left with one vector (the sentence 'idea')
      At which point, end

  
  Test word vectors with diagram of embedding learned (null should not care(?))
  
  See whether grammar has been learned by training a simple POS classifier on 'idea' vectors vs null
    But POS only actually labels word 'ideas'.  
    And these are trained to be insensitive to the current context (i.e. always the same, independent of the current sentence)
    So, in what sense would grammar get adsorbed into the word vectors?
      The tree-ification would have to provide additional information over-and-above the linear structure / local context
      How could that piece be fed as an extra feature alongside the word vectors?
        Perhaps as a simple indicator as to whether the word becomes a left or a right leaf?
          But a lot of that associativity is built into the linear structure already
            Information would have to indicate higher-order associations
            A lot of work would have to be done before this glimmer of light could be unveiled...
  
  Test to see whether parse trees make sense vs null too

  
  ... Time to figure out what 'simple scheme' for the context transformer
  ... (and then the combiner transformer too)
  
  
  Simplest route is to play with Collobert models

    Neural Net version (Cython) (documented, complete with NER examples)
      https://github.com/erickrf/nlpnet
    Neural Net version (Theano) (minimal)
      https://github.com/Fematich/nn_ner

    http://matpalm.com/blog/2015/03/28/theano_word_embeddings/

  Add curruculum idea too
  And cite Klein Manning 2001
  
  What would it mean to train LSTM on sentences 'unsupervised'?
-------------------

  
  
Alternative : 

  DONE 
    Create Entity extraction corpuses from output of :
      Stanford NER
      Berkeley NER

    Then train LSTM network on those corpuses

    Requires out-of-sample validation set 
      and common test sets
    
    Corpus to use : CoNLL-2003, which has NER training, test and 'raw' sets 
      (as well as 1Gb of additional corpus data)
    
    See whether network can (a) replicate and (b) supercede 
    
    Secondary goal : 
      Reduce need to rely on anti-commercial licensed software
    
    Other avenues: 
      Curriculum training based on (say) sentence length
        Or degree of agreement from corpus builders

      Additional NER methods, so that can do voting, and ensembling

    
  DONE :: ICONIP 2016 ? (was ICLR)
    Binary re-representation of word embeddings
    
  Idea :
    Geometry for word analogies, tailored for sparse vectors
      X:Y as A:?
        ? = A+(Y-X)  # clearly
        ? = Y+(A-X)  # in dense space
        ? = (A+Y) -X # in dense space
      
      but want to think only about non-negative 'things'
        need to set up REPL fns to quickly sort through combinations / filters / conjunctions, etc

      regular geometry:
        >>> closest_to(man)
        man, woman, girl, person, men, teenager, she, friend, he, father, her, boy, someone, mother, him, his, victim, son, who, guy
        >>> closest_to(woman)
        woman, man, girl, mother, teenager, daughter, wife, women, her, person, she, girlfriend, friend, men, husband, widow, couple, boy, someone, victim

        >>> closest_to(king)
        king, queen, henry, mswati, mongkut, eirik, charles, vajiravudh, thoden, wenceslaus, zvonimir, athelstan, vladislaus, thelred, gojong, prince, jayavarman, kalkaua, sweyn, pomare
        >>> closest_to(queen)
        queen, princess, elizabeth, king, margrethe, empress, lady, sister, prince, sirikit, mary, cixi, monarch, daughter, duchess, olten, mother, infanta, rania, widow

        >>> count_non_zero(man)
        95
        >>> count_non_zero(woman)
        97
        >>> count_non_zero(king)
        96
        >>> count_non_zero(queen)
        104

        >>> count_non_zero(man+woman)
        147
      
        >>> count_non_zero(man*woman)
        45
        >>> count_non_zero(woman*queen)
        15
        >>> count_non_zero(man*king)
        20
        >>> count_non_zero(king*queen)
        31

        >>> count_non_zero((man+woman)*king)
        24
        >>> count_non_zero(man*king)
        20
        >>> count_non_zero(woman*king)
        17

        >>> count_non_zero(man*woman*king*queen)
        4

        closest_dist('man:woman=king:queen')
        man is to woman as king is to ?queen?
          x+b-a           = king, queen, woman, mswati, vajiravudh, thoden, margrethe, thelred, gojong, mongkut, zvonimir, norodom, wenceslaus, urraca, monarch, hecuba, athelstan, eirik, princess, archelaus
          [x+b-a]         = king, queen, woman, princess, mswati, monarch, henry, vajiravudh, mongkut, eirik, gojong, margrethe, vii, thoden, zvonimir, prince, thelred, norodom, wenceslaus, urraca
          x+[b-a]         = king, woman, queen, henry, princess, monarch, mother, charles, daughter, son, prince, mswati, vii, vajiravudh, mongkut, thelred, louis, eirik, gojong, margrethe
          [x-a]+b         = king, woman, man, queen, girl, mother, daughter, teenager, wife, son, women, her, princess, person, she, monarch, girlfriend, friend, men, husband
          [2x-a]+[2b-a]   = king, woman, queen, daughter, mother, monarch, princess, henry, girl, son, mongkut, mswati, wife, vajiravudh, margrethe, vii, prince, thelred, gojong, eirik
          x+[b-a]+b+[x-a] = king, woman, queen, mother, daughter, man, girl, son, princess, henry, wife, monarch, women, her, charles, she, teenager, prince, mongkut, mswati

        Problem (and insight?)
        >>> closest_dist('pound:england=franc:france')
        pound is to england as franc is to ?france? 
          x+b-a           = england, franc, wales, france, australia, switzerland, scotland, zealand, middlesex, manchester, essex, belgium, capello, portugal, spain, cameroon, lanka, indies, senegal, blanc
          [x+b-a]         = england, franc, wales, france, australia, switzerland, manchester, scotland, essex, middlesex, britain, lancashire, indies, zealand, capello, odi, portugal, cameroon, argentina, francs
          x+[b-a]         = franc, england, francs, wales, australia, peso, switzerland, france, middlesex, rupee, forint, dirham, manchester, odi, indies, dinar, ruble, essex, zealand, currency
          [x-a]+b         = england, franc, wales, australia, france, britain, scotland, switzerland, ireland, manchester, capello, spain, zealand, essex, country, middlesex, africa, lancashire, portugal, indies
          [2x-a]+[2b-a]   = england, franc, wales, australia, france, switzerland, francs, britain, scotland, middlesex, manchester, zealand, essex, indies, capello, odi, lancashire, portugal, spain, swiss
          x+[b-a]+b+[x-a] = franc, england, wales, australia, france, switzerland, francs, britain, scotland, manchester, middlesex, zealand, essex, indies, capello, lancashire, odi, spain, portugal, swiss
  
        Problem:
          The original 3 'base words' are often near the top of the chosen target list
            This happens in the linear (dense) case too
            The original words are deliberately removed from the 'choosable' outcomes
              Otherwise scores are much lower
            But this seems to indicate that the original words have a 'basin/catchment area) that is broader/deeper than the target
            
  
  In-Process  :: To be monetised?
    Palm tree detection

  In-Process  :: To be monetised?
    Identity card reading, from 'bad photos'


  Idea : 
    More labels for better beginning and ending characteristics ?
    Dual LSTM question is pretty-much resolved, though
    Unsupervised chunking?
    
    
  IDEA :
    Middle-out learning of grammar embedding, as above
      Stumbling block is testability of 'segmentation/structure' learned
  
  IDEA :
    Think about 'internal model' method of learning
      (Obviously) read the writing Chinese paper some more
      
  IDEA :
    Character-based NER (done by LSTM paper 2016-03)
  
  IDEA :
    Imagination for visual networks
  
  IDEA : 
    Hierarchical model derived a la InfoGAN
    WordNet is a human artifact
      Perhaps 'reasonable' hierarchies should have similar properties
        eg:  MNIST doesn't have 100 latent variables, nor(?) a binary decision tree
      Look into statistical properties of WordNet (branching factor, depth, etc)
    Are Hierarchical Dirichlet Processes relevant?
   

Side-Note : Pixelwise Segmentation
-------------------------------------------

After lots of fumbling around, it's fairly clear that Theano libraries (Blocks, Lasagne, to name two)
are not really looking at the pixelwise segmentation problem.  And most CNN work doesn't
get there either : what we set out to do simply didn't match the components that others have created.

End result : Either step through the image manually, or use a scatter-gun approach to pick 
patches for training / testing.  It works pretty quickly, even though it doesn't have the appeal of
whole-image-at-a-time training.


Teclast P80 annoyances
-------------------------------------------
http://www.gearbest.com/tablet-pcs/pp_286683.html
https://chiandroid.wordpress.com/category/tablets/teclast/
http://www.aliexpress.com/item/Teclast-P80-3G-Phablet-8-inch-Quad-Core-Android-5-1intel-SoFIA-C3230-64bit-1280-800/32608814119.html?spm=2114.01010208.3.206.3nguKv&ws_ab_test=searchweb201556_7,searchweb201602_1_10037_10017_10021_507_10033_10022_10032_10009_10020_10008_10018_10019,searchweb201603_1&btsid=fbee6553-12a8-4e81-90e2-a9598519076c


http://www.baidu.com/baidu?word=p80+site%3Ateclast.com

http://forums.androidcentral.com/google-nexus-6/502930-phone-keeps-rebooting-optimizing-apps-what-did-i-do-lol-4.html

Reset your Android One to factory settings
  https://support.google.com/android-one/answer/6088915?hl=en

https://www.androidpit.com/how-to-factory-reset-android
http://www.droidviews.com/how-to-boot-android-devices-in-fastboot-download-bootloader-or-recovery-mode/
http://trendblog.net/fix-soft-bricked-android-device-first-aid-guide/
http://trendblog.net/hard-reboot-reset-android/

http://androidforums.com/threads/teclast-p80-tablet.908950/

http://www.chinaphonearena.com/forum/Thread-Teclast-P80-bricked

http://www.teclast.com/tools/pad/showtools.php

http://bbs.teclast.com/forum-93-1.html
  http://bbs.teclast.com/thread-392079-1-1.html
  http://bbs.teclast.com/thread-255945-1-1.html
  http://bbs.teclast.com/thread-510532-1-1.html
  http://bbs.teclast.com/thread-258622-1-2.html


http://techtablets.com/forum/topic/teclast-official-firmware-repository/


2015-12-14
  P80 3G (K3H5) Android 5.1 firmware download
  V1.05_20151204 (android5.1)
  
  "telecommunications"
    P80 3G (K3H5) -Android5.1-V1.05.rar
      http://pan.baidu.com/s/1kUj2cx5
      File size: 487.6M

  "netcom"
    P80 3G (K3H5)-Android5.1-V1.05.rar
      http://pan.baidu.com/s/1kUj2cx5
      Gives up at : 524.3 kB (524,288 bytes)    

