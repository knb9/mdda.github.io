% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%  
\documentclass{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
%
\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads
\addtocmark{AGI Building Blocks} % additional mark in the TOC
%
\mainmatter              % start of the contributions
%
\title{AGI Building Blocks : The first 100 steps}
%
\titlerunning{AGI Building Blocks}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Martin Andrews\inst{1}}
%
\authorrunning{Martin Andrews} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Martin Andrews}
%
\institute{Red Cat Labs, Singapore, \\
\email{Martin.Andrews@RedCatLabs.com},\\ 
WWW home page: \texttt{http://RedCatLabs.com/}
}

\maketitle              % typeset the title of the contribution

%%%% using at least 70 and at most 150 words. It will be set in 9-point
\begin{abstract}
In an interview during AGI 2014, Joscha Bach made an interesting comment that 
building AGI would be the result of (ballpark) 100 different organizational principles - 
with Backpropagation being an example of a principle that could be on that list.  
This discussion paper outlines a few more candidate building blocks, 
while suggesting that Backpropagation is unlikely to be,  in itself, foundational.  
\dots
\keywords{building blocks, backpropagation, deep learning, hype}
\end{abstract}
%

\section{Background}
The study of machine intelligence has suffered from a number of false dawns, 
as practitioners at the forefront of the latest wave (dubbed 'deep learning') 
are all too aware.
However, despite their caution, it remains difficult to restrain reporters
who want to marvel at the latest 'magic' being produced by AI research.

In order to set the current wave in context, Joscha Bach 
( at AGI 2012 in Oxford 12m38s - 13m48s )  commented that :


\begin{quotation}
%nature has managed to do it
%we don't know how the software should be written - 
%we have some good pointers though.

I think that 
eventually it will boil down to a few hundred organisational principals
\dots

%that can be encoded in the genome
%and the upper complexity boundary for 
%this is probably the information content of the genome
%which, famously, fits on a CDROM.

%So actually our blueprint for our body is simpler than
%the blueprint for a current edition of Microsoft Windows.  
%But it has not been built to be re-engineered.  

It is very difficult to find the organising principles.

And it's often very easy to see them in hind-sight : 
Backpropagation learning, with hindsight, is a very, very simple phenomenon, 
but it took mankind a  very long time to hit upon the principle of Backpropagation learning. 

I guess we have a few hundred problems on the difficulty level of Backpropagation learning to solve before we get to AGI. 

%And so, in my perspective, we have many, many PhD students that need to get engaged
%with this and combine this into a common platform - I don't see any easy shortcuts.

\end{quotation}

This paper argues that several other building blocks are also evident, 
that backpropagation itself might be considered to be several related pieces of the puzzle,
and that current Machine Learning research, while exciting, may not be
discovering new blocks systematically.

\subsection{Overview}

Deep Learning : 
  New crop of low-hanging fruit 
    ref: previous AI hype that lead to winters after picking the low-hanging fruit

back-propagation : what it is, and what it isn't
  gradient descent is just a method for energy minimisation
    there are lots of interesting speed-ups (momentum, AGAgrad, etc)
      but biological plausibility already left the building
    actual goal is finding lower-energy states
      update methods and enhancements are techniques, not essential elements
      
  many other methods exist for minimisation
    eg: WTF-propagation : random matrix error propagation (Lillicrap 2014)
      possible to use 'structured' random matrices that are compute-friendly?
      suggests that brute-force computation beats elegant mathematics
      
  looking at, for instance, whitening : this is a bigger win than fancier gradient descent methods
    shows that engineering for efficiency may be missing bigger picture
    could whitening applied to old-generation models actually work?
  
drop-out is an interesting technique that deserves to be investigated in itself
  possible to use 'structured' dropout that is more compute-friendly?
  
ensemble methods
  this has a direct mathematical underpinning
  can't ensemble with single learned core - no diversity created
  
sparse encodings
  relatively unexplored, compared to the use that the brain appears to make of it
    may be an artifact of brain's components
    but, looking at word-embedded (whether as float-vectors, or as coincidence matrices)
      sparse pattern matching is more 'unfairly' powerful 
      random projection

Evolution
  Works 'unfairly well'
  Distance from computation
    GAs operate directly (parameters for a given substrate)
    GPs operate (parameters that directly create a substrate)
    DNA : code that creates stuff that self-organizes to create conditions where 
      other parts of DNA interact, to produce...   
        final result is third / forth hand from inital representation

internal representations
  cross-domain knowledge
    already demonstrated that internal representations can be re-used effectively
    another 'unfair advantage'

higher level human understanding of :
  mathematics
    energy minimisation
    minimum description length
  
  systems thinking
    hierarchies of concepts
      system goals of brain/neurons not necessarily optimal for conciousness
    can be a trap 
      'thoughts' do can flow fluidly - no need for logical construction
        in retrospect, logic can be found
      mass experimentation (at forefront of Deep Learning, for instance) beneficial
        things that work unfairly well can be systematised/rationalised later

  symbolic reasoning
    brain has a single substrate, organized 'third hand' by DNA
    AGI designers operate more hands-on
      constrained by iteration speed (new structures)
      constrained by local search (new ideas)

system goals of brain/neurons not necessarily optimal for conciousness
  brain 
    robustness
    lower power
    requirements of modern society (much higher reasoning ability) too recent for evolution
    3d structure
    approximate replicability
    wasteful approaches to computation may still be a 'win' in other aspects 
      (power, speed, robustness, style of convergence, communicability, etc)
    
  machine
    repeatability
    precision (available)
    ability to make use of power-wasteful structures (can re-engineer heat flow)
    

conciousness
  still no idea where to start
  difference between making a dog and making a person
    capability for sophisticated language model
      markov state + RNN + beam search -> plausible linguistic output
  
  stories that we tell ourselves
    narrative that we are aware of is highly edited version of sub
      'top of mind' is what floats to the surface
      deep sea of unconcious though seething away beneath
  introspection necessarily can't get beneath the surface

unsupervised learning is actually weakly supervised 
  all training examples are positive (compared to non-training examples)
  JPEG is good compression of visual images
    suggests that we might train on that, since it's already semi-optimised for visual field

organisation of interesting test cases is biased
  humans perceive objects that have relationships and labels on different levels
    these tasks are implicitly also human-learnable
    properties that are useful to neural network learning (priors, causality, shared factors) are embedded already
  no interest in classifying (for instance) different 'random' images

PCA is a 'high level' concept
  but can be implemented using simple circulation of error terms
  efficient compute implementation uses features of silicon unavailable to biological neurons


fertle / toofer / turst exercise in common features biological botanical
  where did I see this?
  




\paragraph{Notes and Comments.}

have obtained lower bound on the number of subharmonics of period $kT$,
based on symmetry considerations and on pinching estimates, as in
Sect.~5.2 of this article.


%
% ---- Bibliography ----
%
\begin{thebibliography}{5}
%
\bibitem {clar:eke}
Clarke, F., Ekeland, I.:
Nonlinear oscillations and
boundary-value problems for Hamiltonian systems.
Arch. Rat. Mech. Anal. 78, 315--333 (1982)

\bibitem {clar:eke:2}
Clarke, F., Ekeland, I.:
Solutions p\'{e}riodiques, du
p\'{e}riode donn\'{e}e, des \'{e}quations hamiltoniennes.
Note CRAS Paris 287, 1013--1015 (1978)

\bibitem {mich:tar}
Michalek, R., Tarantello, G.:
Subharmonic solutions with prescribed minimal
period for nonautonomous Hamiltonian systems.
J. Diff. Eq. 72, 28--55 (1988)

\bibitem {tar}
Tarantello, G.:
Subharmonic solutions for Hamiltonian
systems via a $\bbbz_{p}$ pseudoindex theory.
Annali di Matematica Pura (to appear)

\bibitem {rab}
Rabinowitz, P.:
On subharmonic solutions of a Hamiltonian system.
Comm. Pure Appl. Math. 33, 609--633 (1980)

\end{thebibliography}

\end{document}
