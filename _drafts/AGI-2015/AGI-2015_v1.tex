% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%  
\documentclass[citeauthoryear]{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
%
\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads
\addtocmark{Lessons from Deep Learning} % additional mark in the TOC
%
\mainmatter              % start of the contributions
%
\title{Lessons from Deep Learning}
%
\titlerunning{Lessons from Deep Learning}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Martin Andrews\inst{1}}
%
\authorrunning{Martin Andrews} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Martin Andrews}
%
\institute{Red Cat Labs, Singapore, \\
\email{Martin.Andrews@RedCatLabs.com},\\ 
WWW home page: \texttt{http://RedCatLabs.com/}
}

\maketitle              % typeset the title of the contribution

%%%% using at least 70 and at most 150 words. It will be set in 9-point
\begin{abstract}
%In an interview \cite{Joscha-Bach-2012-interview} during AGI 2012, Joscha Bach made an interesting comment that 
%the process of achieving AGI will likely require the discovery of its `100 organizational principles'.
%Those principles may be obvious in hindsight, but (using the example of Backpropagation learning)
%may take a long time for mankind to discover.
%This short communication outlines a few more candidate building blocks to put on the list, 
%while discussing which apects of Backpropagation mean that it deserves to be included.  

%The complexity of each organizational principle might be of the order of Backpropagation learning - 

%  What deep learning can tell us about finding organizational principles

In an interview  during AGI 2012, Joscha Bach (\cite{Joscha-Bach-2012-interview}) 
made an interesting comment that the process of achieving 
AGI will likely require the discovery of its `100 organizational principles' - 
using Backpropagation learning as an example of one of those principles.  

This short communication argues that the lessons to be learned from 
the successes of deep learning research are not as clear-cut as they might first appear,
because the field is still in a rapid exploration phase, and `conventional wisdom' 
being rewritten regularly.   In addition, away from the lime-light surrounding 
the production of headline results for competitive benchmarks, 
there may still be much to be gained from examining some of 
the field's basic questions.

\keywords{deep learning, backpropagation, organisational principles}
\end{abstract}
%
\section{Background}
The study of machine intelligence has suffered from a number of false dawns, 
as practitioners at the forefront of the latest wave (dubbed `deep learning') 
are keenly aware.  

This paper argues that several other building blocks are also evident, 
that backpropagation itself might be considered to be several related pieces of the puzzle,
and that the emphasis of Deep Learning research on competitive test sets, 
while exciting, may not be discovering new blocks systematically.

%Potentially : 
%  Reformulate as to how these steps should inform our search for relevant methods
%  Brute force can still benefit from 10x speedups
%    `Just' needs an understanding of why things work, so that can apply relevant higher mathematics
  
% **  What deep learning can tell us about finding organizational principles ** 

% 4:25 - 6:12  (1:47)

%\section{Overview}
%
%Short "technical communications", with a limit of 4 pages, 
%summarizing results and ideas of interest to the AGI audience, 
%including : 
%  reports about recent publications, 
%  position papers, and 
%  preliminary results.
%
%Appropriate topics for contributed papers include, but are not restricted to:
%  Creativity
%  Knowledge Representation for General Intelligence
%  Learning, and Learning Theory
%  Natural Language Understanding
%  Neural-Symbolic Processing
%  Philosophy of AGI
%  Reinforcement Learning
%
%Submission deadline moved from 15-march-2015 to 1-apr-2015
%  :: http://agi-conf.org/2015/call-for-papers/

\section{Deep Learning Lessons}
Previous AI research has shown that new techniques allow new areas 
of `intelligent behaviour' to be rapidly brought into the domain of 
problems that are tractable by computer methods.  However, AI has been 
cursed by hype, whereby these problems are soon discovered to 
be `low-hanging fruit' compared to large crop required by the 
overall AGI endevour.

With recent advances in Deep Learning beginning to show better-than-human 
levels of perfomance in image classification, it is tempting to believe 
that other hurdles will be quickly overcome.  However, it is worth considering
what lessons can be learned from the Deep Learning AI sub-genre, which has 
had its own `winters' since Neural Networks were first fashionable in 
the 1980s.  For simplicity, the `early days' will refer to the 1980s/early-1990s,
the `new phase' as 2006 onwards, and `recently' to 2011 to the present 
(with much respect to those who founded the field earlier, and also 
those who soldiered on during the in-between times) - for a comprehensive 
overview, see Schmidhuber (\cite{SchmidhuberOverview}).


\subsubsection*{Did we give up too early?}
 
One feature of today's Deep Learning is that large models are being learned
from huge datasets by machines that are orders of magnitude faster than 
were available in the early 1990s - and it is argued that the speed 
differential is one of the main reasons that progress is being made now.

This argument would be more compelling if it had been observed at the time 
that the experiments were working, but much too slowly.  Often, large (downward)
jumps in error are experienced in early training epochs in today's experiments.  
Those jumps should have been observable even if the experiment couldn't have 
been run to convergence in the past.  

So maybe the reasoning is more nuanced : It is not just the raw learning speed 
for a given experiment that is important, but the iteration / experiment cycle time.
So that the model discovery process can run in reasonable time, whereas before
it was truncated before useful conclusions could be made.

Simply having more data available during the `new phase' 
(The Unreasonable Effectiveness of Data (Halevy, Norvig \& Pereira, \cite{norvig-UnreasonableEffectivenessOfData})
is also a factor - as is the bravado to actually try to make sense of it all.


\subsubsection*{Random Propagation.}

While the basic backpropagation is easily understood as the application 
of the chain rule backwards through the network, it is rarely used in pure form
because various speed-up methods (such as momentum terms, ADAgrad, RMSprop, etc)
are almost a requirement for reasonable training times.  

While some of these methods have their roots in previous optimisation theory, 
others have been developed by `seeing what works' - and have shown that 
learning occurs even with (for instance) random matrix error propagation 
(Lillicrap \cite{Lillicrap-random-matrix}).

The robustness of the backpropation algorithms used to (effectively) `helpful noise'
suggests that brute-force computation can beat elegant mathematics, and that 
the brain doesn't need to be able to implement an optimal strategy - just one that works.


\subsubsection*{Exploding gradients.}

Even though the basic operation of a layer (matrix multiple and non-linearity) 
have been apparent since the beginning, it is only relatively recently that 
techniques have been uncovered that mitigate the problem : 
\begin{description}
\item[Rectified Linear Units (ReLU) layers] - by computing output vectors $y$ 
as $y=max(Wx+b, 0)$, this simply avoids scaling problems by having the non-linearity agnostic to scale.
\item[Informed intialisation schemes] - network weights are initialised 
to random values that are scaled based on fan-in/fan-out considerations as a way of making the 
average scaling-factor of the matrix multiplication closer to unity.
\item[Normalization] - the scaling of the layer outputs is controlled
dynamically (described in G{\"u}l{\c{c}}ehre and Bengio, \cite{bengio-whitening} and 
Ioffe \& Szegedy, \cite{WhiteningOfData})
\end{description}

In retrospect, the fact that simple Genetic Algorithms (GAs) could train 
Neural Networks roughly as quickly as gradient descent during the early days, 
should have been telling : With exact gradients, local search should have 
been much more effective than any global search method.


\subsubsection*{PCA and Friends.}

Prinicipal Component Analysis (PCA) is a `high level' mathematical concept, 
that can, nevertheless, be implemented using simple circulation of error terms
in a biologically realistic setting (see Harpur (\cite{harpur-thesis})).
It can also be used as a realistic base-line against which to compare more
elaborate neural network models (Chan et al (\cite{PCAnet})).

However, techniques such as Singular Value Decomposition (SVD) can more 
efficiently compute the same results using features of silicon unavailable to biological neurons.

Sparse PCA is a step in the right direction, but may be optimising the wrong sparsity.
 

\subsubsection*{Natural Problems aren't Antagonistic.}

In the early days, Neural Network researchers spent a lot of time on the 
tractability of the XOR problem, along with other toy problems that were 
intentionally made challenging for networks to learn.

More recently, as demonstrated repeatedly, when presented with `natural' problems, 
even unsupervised learning approaches have shown that real-world data sets 
are not as antagonistic to being learned as might have been anticipated.  
This might illustrate that the priors embedded in network learning are 
somehow sympathetic to the data being learned 
(see, for instance, "Priors Regarding The Underlying Factors" in Bengio et al. \cite{Bengio-et-al-2014-Book}),
or that the interesting features that humans themselves have identified as being 
worth learning are those that `come easily' to human brains themselves.

Label sparsity is something that likely occurs at multiple levels in the brain 
(rather than just at the output layer) and is also in contrast to the 
nuanced internal representations that a simple PCA approach might suggest.


\subsubsection*{Convolutional Nets (human-assisted representation).}

The formulation of Convolution Networks is also part of the new movement, 
and it is interesting because it exposes a useful representational property of images.  
But it is somewhat hiding the fact that human knowledge is being implicitly 
built into the network via the representation scheme.

The flip side of this is that the convolutional layers seem to 
map (loosely) to the different levels of processing within the brain - 
in a way that is highly suggestive of having explanatory power.

 
\section{Deep Learning New Stuff}

The following can all be characterized as `unfairly effective', and have 
arisen as part of the rapid exploration that characterises Deep Learning research.

\subsubsection*{Word Vector Embedding.}

    but, looking at word-embedded (whether as float-vectors, or as coincidence matrices)
      sparse pattern matching is more `unfairly' powerful 
      random projection

\subsubsection*{Transfer Learning / Internal Representations.}
Taking a cross-section through a trained network to see its internal representation
has been shown to be applicable to other problem domains (Karpathy \& Fei-Fei, \cite{karpathy2014deep}).

The usefulness of these internal representations
\footnote{ ... which could also be thought of as 'ideas', particularly 
if there were a means to capture transitions between them, aka `stories'.}
has powerful implications regarding the power of networks trained by 
different modalities to enhance each other's effectiveness without the 
brittleness associated with building a single 'monolithic' system.


\subsubsection*{Drop-out.}
this training enhancement (Hinton et al, (\cite{hinton-dropout})) 
can be rationalised as allowing multiple competitive ensembles to self-organize.  
However, despite the essence of its operation not being fully understood, 
many researches are using it as part of the standard toolkit without 
exploring why it works.


\subsubsection*{Trailblazing.}
The Deep Learning field appears wide open to new ideas.  
Practitioners are free to blaze a new trail, and this is sometimes 
reincorporated into the existing frameworks once researchers in those 
fields realize that something previously unthinkable actually works.


\section{Other Organisational Principles}

Here are some of the key organisational principles that seem appropriate 
to label `foundational' to the cause of AGI :
\begin{description}
\item[Ensemble Methods] - whereby an ensemble of different approaches to solving a problem to rise above the accuracy 
of any one method has a simple mathematical underpinning.  
\item[Sparse Encodings] - relatively unexplore compared to the use that the brain appears to make of them
(see also : priors)
\item[Evolutionary Methods] - currently appear to be suffering through a winter of their own, 
despite working `unfairly well'
\item[Minimum Description Length] - a concept-cluster that extends from compression, 
to information theory, encompassing Occam's razor along the way.
\end{description}


\section{The Known Unknown}

conciousness 
  still no idea where to start
  difference between making a dog and making a person
    capability for sophisticated language model
      markov state + RNN + beam search == plausible linguistic output
  
  stories that we tell ourselves
    narrative that we are aware of is highly edited version of sub
      `top of mind' is what floats to the surface
      deep sea of unconscious though seething away beneath
  introspection necessarily can't get beneath the surface

%
% ---- Bibliography ----
%
\begin{thebibliography}{}
%
% Fix the tilde...
\newcommand{\mytilde}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}

\bibitem[2012]{Joscha-Bach-2012-interview}
Joscha Bach:
Interview during AGI 2012
Available on Youtube, 12m38s - 13m48s (2012)

\bibitem[2014]{Bengio-et-al-2014-Book}
Yoshua Bengio and Ian J. Goodfellow and Aaron Courville:
Deep Learning.
MIT Press (2014, book in preparation)
% http://www.iro.umontreal.ca/~bengioy/dlbook/disentangling.html
(http://www.iro.umontreal.ca/\mytilde{}bengioy/dlbook)

\bibitem[2014]{PCAnet}
Tsung-Han Chan, Kui Jia, Shenghua Gao, Jiwen Lu, Zinan Zeng, Yi Ma:
PCANet: A Simple Deep Learning Baseline for Image Classification?
eprint arXiv:1404.3606v2 (2014)

\bibitem[2013]{bengio-whitening}
G{\"u}l{\c{c}}ehre, {\c{C}}a{\u{g}}lar and Bengio, Yoshua:
Knowledge matters: Importance of prior information for optimization.
CoRR, abs/1301.4083 (2013).

\bibitem[2009]{norvig-UnreasonableEffectivenessOfData}
Alon Halevy, Peter Norvig, Fernando Pereira:
The Unreasonable Effectiveness of Data.
IEEE Intelligent Systems, vol. 24 (2009), pp. 8-12

\bibitem[1997]{harpur-thesis}
Harpur, George Francis: 
Low entropy coding with unsupervised neural networks
Dissertation, University of Cambridge (1997).

\bibitem[2012]{hinton-dropout}
Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R:
Improving neural networks by preventing co-adaptation of feature detectors
arXiv preprint arXiv:1207.0580 (2012)

\bibitem[2015]{WhiteningOfData}
Sergey Ioffe, Christian Szegedy:
Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
eprint arXiv:1502.03167v3 (2015)

\bibitem[2014]{karpathy2014deep}
Karpathy, Andrej and Fei-Fei, Li:
Deep visual-semantic alignments for generating image descriptions
arXiv preprint arXiv:1412.2306 (2014)

\bibitem[2014]{Lillicrap-random-matrix}
Lillicrap, Timothy P.; Cownden, Daniel; Tweed, Douglas B.; Akerman, Colin J.:
Random feedback weights support learning in deep neural networks.
eprint arXiv:1411.0247

\bibitem[2014]{SchmidhuberOverview}
Juergen Schmidhuber:
Deep Learning in Neural Networks: An Overview
eprint arXiv:1404.7828v4 (2014)

%\bibitem {clar:eke}
%Clarke, F., Ekeland, I.:
%Nonlinear oscillations and
%boundary-value problems for Hamiltonian systems.
%Arch. Rat. Mech. Anal. 78, 315--333 (1982)
%
%\bibitem {clar:eke:2}
%Clarke, F., Ekeland, I.:
%Solutions p\'{e}riodiques, du
%p\'{e}riode donn\'{e}e, des \'{e}quations hamiltoniennes.
%Note CRAS Paris 287, 1013--1015 (1978)
%
%\bibitem {mich:tar}
%Michalek, R., Tarantello, G.:
%Subharmonic solutions with prescribed minimal
%period for nonautonomous Hamiltonian systems.
%J. Diff. Eq. 72, 28--55 (1988)
% 
%\bibitem {tar}
%Tarantello, G.:
%Subharmonic solutions for Hamiltonian
%systems via a $\bbbz_{p}$ pseudoindex theory.
%Annali di Matematica Pura (to appear)
%
%\bibitem {rab}
%Rabinowitz, P.:
%On subharmonic solutions of a Hamiltonian system.
%Comm. Pure Appl. Math. 33, 609--633 (1980)

\end{thebibliography}

\newpage

\section{OLD Background}
The study of machine intelligence has suffered from a number of false dawns, 
as practitioners at the forefront of the latest wave (dubbed `deep learning') 
are keenly aware.  
%
%However, despite their caution, it remains difficult to restrain reporters
%who want to marvel at the latest `magic' being produced by AI research.

In order to set the current wave in context, Joscha Bach \cite{Joscha-Bach-2012-interview}
commented that :

\begin{quotation}
%nature has managed to do it
%we don't know how the software should be written - 
%we have some good pointers though.

I think that eventually it will boil down to a few hundred organisational principals
\dots
%
%that can be encoded in the genome
%and the upper complexity boundary for 
%this is probably the information content of the genome
%which, famously, fits on a CDROM.
%
%So actually our blueprint for our body is simpler than
%the blueprint for a current edition of Microsoft Windows.  
%But it has not been built to be re-engineered.  
%
(However) it is very difficult to find the organising principles.

And it's often very easy to see them in hindsight : 
Backpropagation learning, with hindsight, is a very, very simple phenomenon, 
but it took mankind a very long time to hit upon the principle of Backpropagation learning. 

I guess we have a few hundred problems on the difficulty level of Backpropagation learning to solve before we get to AGI. 

%And so, in my perspective, we have many, many PhD students that need to get engaged
%with this and combine this into a common platform - I don't see any easy shortcuts.

\end{quotation}

This paper argues that several other building blocks are also evident, 
that backpropagation itself might be considered to be several related pieces of the puzzle,
and that current Machine Learning research, while exciting, may not be
discovering new blocks systematically.



\subsubsection*{Ensemble Methods.}
The ability of an ensemble of different approaches to solving a problem to rise above the accuracy 
of any one method has a simple mathematical underpinning.  

Can't make an ensemble with single learned core - no diversity created

drop-out is an interesting technique that deserves to be investigated in itself
  possible to use `structured' dropout that is more compute-friendly?


\subsubsection*{Sparse Encodings.}
  relatively unexplored, compared to the use that the brain appears to make of it
    may be an artifact of brain's components
    
\subsubsection*{Evolution.}
  Works `unfairly well'
  Distance from computation
    GAs operate directly (parameters for a given substrate)
    GPs operate (parameters that directly create a substrate)
    DNA : code that creates stuff that self-organizes to create conditions where 
      other parts of DNA interact, to produce...   
        final result is third / forth hand from inital representation

\subsubsection*{Minimum Description Length.}
This concept-cluster is another mathematical one : extending from compression, to information theory,
encompassing Occam's razor along the way.

\subsubsection*{Stories.}
  ???


\section{BackPropagation Thoughts}
  gradient descent is just a method for energy minimisation
    there are lots of interesting speed-ups (momentum, AGAgrad, etc)
      but biological plausibility already left the building
      
    actual goal is finding lower-energy states
      update methods and enhancements are techniques, not essential elements



unsupervised learning is actually weakly supervised 

  humans perceive objects that have relationships and labels on different levels
    these tasks are implicitly also human-learnable
    properties that are useful to neural network learning (priors, causality, shared factors) are embedded already
    
  no interest in classifying (for instance) different `random noise' images
  
  features that are interesting to humans seem to be either fairly independent or subsetting
    unlikely to have interesting features that are complicated vectors of properties
    this is an interesting prior that can be imposed on a network

\end{document}


