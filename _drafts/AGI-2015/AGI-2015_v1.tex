% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%  
\documentclass{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
%
\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads
\addtocmark{AGI Building Blocks} % additional mark in the TOC
%
\mainmatter              % start of the contributions
%
\title{AGI Building Blocks : The first 100 steps}
%
\titlerunning{AGI Building Blocks}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Martin Andrews\inst{1}}
%
\authorrunning{Martin Andrews} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Martin Andrews}
%
\institute{Red Cat Labs, Singapore, \\
\email{Martin.Andrews@RedCatLabs.com},\\ 
WWW home page: \texttt{http://RedCatLabs.com/}
}

\maketitle              % typeset the title of the contribution

%%%% using at least 70 and at most 150 words. It will be set in 9-point
\begin{abstract}
In an interview during AGI 2014, Joscha Bach made an interesting comment that 
building AGI would be the result of (ballpark) 100 different new ideas - 
with Backpropagation being an example idea on that list.  This discussion paper 
outlines a few more candidate building blocks, while putting Backpropagation 
in context
\dots
\keywords{building blocks, backpropagation, deep learning, hype}
\end{abstract}
%

\section{Background}
The study of machine intelligence has suffered from a number of false dawns, 
as practitioners at the forefront of the latest wave (dubbed 'deep learning') 
are all too aware.
However, despite their caution, it remains difficult to restrain reporters
who want to marvel at the latest 'magic' being produced by AI research.

In order to set the current wave in context, Joscha Bach commented that 
In order to set the current wave in context, Joscha Bach commented that 
In order to set the current wave in context, Joscha Bach commented that 


\begin{quotation}
something something something
\end{quotation}

This paper argues that several other building blocks are also evident, 
that backpropagation itself might be considered to be several related pieces of the puzzle,
and that current Machine Learning research, while exciting, may not be
discovering new blocks systematically.

\subsection{Autonomous Systems}

SAJSKDJASDS


\paragraph{Notes and Comments.}

have obtained lower bound on the number of subharmonics of period $kT$,
based on symmetry considerations and on pinching estimates, as in
Sect.~5.2 of this article.


%
% ---- Bibliography ----
%
\begin{thebibliography}{5}
%
\bibitem {clar:eke}
Clarke, F., Ekeland, I.:
Nonlinear oscillations and
boundary-value problems for Hamiltonian systems.
Arch. Rat. Mech. Anal. 78, 315--333 (1982)

\bibitem {clar:eke:2}
Clarke, F., Ekeland, I.:
Solutions p\'{e}riodiques, du
p\'{e}riode donn\'{e}e, des \'{e}quations hamiltoniennes.
Note CRAS Paris 287, 1013--1015 (1978)

\bibitem {mich:tar}
Michalek, R., Tarantello, G.:
Subharmonic solutions with prescribed minimal
period for nonautonomous Hamiltonian systems.
J. Diff. Eq. 72, 28--55 (1988)

\bibitem {tar}
Tarantello, G.:
Subharmonic solutions for Hamiltonian
systems via a $\bbbz_{p}$ pseudoindex theory.
Annali di Matematica Pura (to appear)

\bibitem {rab}
Rabinowitz, P.:
On subharmonic solutions of a Hamiltonian system.
Comm. Pure Appl. Math. 33, 609--633 (1980)

\end{thebibliography}

\end{document}
