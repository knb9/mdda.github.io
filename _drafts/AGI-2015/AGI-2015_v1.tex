% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%  
\documentclass{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
%
\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads
\addtocmark{AGI Building Blocks} % additional mark in the TOC
%
\mainmatter              % start of the contributions
%
\title{AGI Building Blocks : The first 100 steps}
%
\titlerunning{AGI Building Blocks}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Martin Andrews\inst{1}}
%
\authorrunning{Martin Andrews} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Martin Andrews}
%
\institute{Red Cat Labs, Singapore, \\
\email{Martin.Andrews@RedCatLabs.com},\\ 
WWW home page: \texttt{http://RedCatLabs.com/}
}

\maketitle              % typeset the title of the contribution

%%%% using at least 70 and at most 150 words. It will be set in 9-point
\begin{abstract}
%In an interview \cite{Joscha-Bach-2012-interview} during AGI 2012, Joscha Bach made an interesting comment that 
%the process of achieving AGI will likely require the discovery of its `100 organizational principles'.
%Those principles may be obvious in hindsight, but (using the example of Backpropagation learning)
%may take a long time for mankind to discover.
%This short communication outlines a few more candidate building blocks to put on the list, 
%while discussing which apects of Backpropagation mean that it deserves to be included.  

%The complexity of each organizational principle might be of the order of Backpropagation learning - 

%  What deep learning can tell us about finding organizational principles

In an interview \cite{Joscha-Bach-2012-interview} during AGI 2012, 
Joscha Bach made an interesting comment that the process of achieving 
AGI will likely require the discovery of its `100 organizational principles', 
using Backpropagation learning as an example of one of these organisational principles.  

This short communication argues that the lessons to be learned from 
the latest deep learning research are not as clear-cut as they might first appear,
and that the field is still in a rapid exploration phase - with the accepted wisdom 
being regularly challenged.

Moreover, there is still much knowledge to be gained from examining some of the 
basic questions in the deep learning field, away from the lime-light that 
the benchmark-competition headline results.

\keywords{building blocks, organisational principles, backpropagation, deep learning}
\end{abstract}
%
\section{Background}
The study of machine intelligence has suffered from a number of false dawns, 
as practitioners at the forefront of the latest wave (dubbed 'deep learning') 
are keenly aware.  

This paper argues that several other building blocks are also evident, 
that backpropagation itself might be considered to be several related pieces of the puzzle,
and that current Machine Learning research, while exciting, may not be
discovering new blocks systematically.


Potentially : 
  Reformulate as to how these steps should inform our search for relevant methods
  Brute force can still benefit from 10x speedups
    'Just' needs an understanding of why things work, so that can apply relevant higher mathematics
  
  What deep learning can tell us about finding organizational principles


%\section{Overview}
%
%Short "technical communications", with a limit of 4 pages, 
%summarizing results and ideas of interest to the AGI audience, 
%including : 
%  reports about recent publications, 
%  position papers, and 
%  preliminary results.
%
%Appropriate topics for contributed papers include, but are not restricted to:
%  Creativity
%  Knowledge Representation for General Intelligence
%  Learning, and Learning Theory
%  Natural Language Understanding
%  Neural-Symbolic Processing
%  Philosophy of AGI
%  Reinforcement Learning
%
%Submission deadline moved from 15-march-2015 to 1-apr-2015
%  :: http://agi-conf.org/2015/call-for-papers/

\section{Organisational Principles}

Here are some of the key organisational principles that seem appropriate to label 'foundational' :

\subsubsection*{Ensemble Methods.}
The ability of an ensemble of different approaches to solving a problem to rise above the accuracy 
of any one method has a simple mathematical underpinning.  

Can't make an ensemble with single learned core - no diversity created

drop-out is an interesting technique that deserves to be investigated in itself
  possible to use 'structured' dropout that is more compute-friendly?


\subsubsection*{Sparse Encodings.}
  relatively unexplored, compared to the use that the brain appears to make of it
    may be an artifact of brain's components
    
\subsubsection*{Word Embeddings.}
    but, looking at word-embedded (whether as float-vectors, or as coincidence matrices)
      sparse pattern matching is more 'unfairly' powerful 
      random projection

\subsubsection*{Convolutions.}
don't seem foundational, but for the fact that it exposes a useful representational property

also interesting that convolutional layers seem to map (loosely) on the 
different levels of processing within the brain.

\subsubsection*{Evolution.}
  Works 'unfairly well'
  Distance from computation
    GAs operate directly (parameters for a given substrate)
    GPs operate (parameters that directly create a substrate)
    DNA : code that creates stuff that self-organizes to create conditions where 
      other parts of DNA interact, to produce...   
        final result is third / forth hand from inital representation

\subsubsection*{Internal Representations.}
  cross-domain knowledge
    already demonstrated that internal representations can be re-used effectively
    another 'unfair advantage'
  Also a potential unit for an 'idea'
    need to work on transitions between ideas : Stories
      ... and then we'd really be getting somewhere

\subsubsection*{energy minimisation}
PCA is a 'high level' concept
  but can be implemented using simple circulation of error terms
  efficient compute implementation uses features of silicon unavailable to biological neurons

\subsubsection*{Minimum Description Length.}
This concept-cluster is another mathematical one : extending from compression, to information theory,
encompassing Occam's razor along the way.

\subsubsection*{Stories.}


\subsubsection*{Implicit Priors.}
unsupervised learning is actually weakly supervised 
  humans perceive objects that have relationships and labels on different levels
    these tasks are implicitly also human-learnable
    properties that are useful to neural network learning (priors, causality, shared factors) are embedded already
  no interest in classifying (for instance) different 'random noise' images
  features that are interesting to humans seem to be either fairly independent or subsetting
    unlikely to have interesting features that are complicated vectors of properties
    this is an interesting prior that can be imposed on a network
      "Priors Regarding The Underlying Factors" : Section 14.5 of Bengio deep learning book (in preview) 
        \cite{Bengio-et-al-2014-Book} 




\section{BackPropagation Thoughts}
  gradient descent is just a method for energy minimisation
    there are lots of interesting speed-ups (momentum, AGAgrad, etc)
      but biological plausibility already left the building
      
    actual goal is finding lower-energy states
      update methods and enhancements are techniques, not essential elements

\subsubsection*{Network Whitening.}
    shows that engineering for efficiency may be missing bigger picture
    could whitening applied to old-generation models actually work?

\subsubsection*{Random Propagation.}
    eg: WTF-propagation : random matrix error propagation (Lillicrap 2014)
      possible to use 'structured' random matrices that are compute-friendly?
      suggests that brute-force computation beats elegant mathematics
        brain doesn't need to implement best strategy - just one that works

\subsubsection*{Drop-out.}
    something is going on here that is not really understood
    
\subsubsection*{Deep Learning.}
  New crop of low-hanging fruit 
    ref: previous AI hype that lead to winters after picking the low-hanging fruit


\section{The Known Unknown}

conciousness 
  still no idea where to start
  difference between making a dog and making a person
    capability for sophisticated language model
      markov state + RNN + beam search == plausible linguistic output
  
  stories that we tell ourselves
    narrative that we are aware of is highly edited version of sub
      'top of mind' is what floats to the surface
      deep sea of unconscious though seething away beneath
  introspection necessarily can't get beneath the surface

%
% ---- Bibliography ----
%
\begin{thebibliography}{5}
%


\bibitem {Bengio-et-al-2014-Book}
Yoshua Bengio and Ian J. Goodfellow and Aaron Courville:
Deep Learning.
Book in preparation for MIT Press (2014)
% Fix the tilde...
\newcommand{\mytilde}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}
% http://www.iro.umontreal.ca/~bengioy/dlbook/disentangling.html
(http://www.iro.umontreal.ca/\mytilde{}bengioy/dlbook)

\bibitem {Joscha-Bach-2012-interview}
Joscha Bach:
Interview during AGI 2012
Available on Youtube, 12m38s - 13m48s (2012)


\bibitem {clar:eke}
Clarke, F., Ekeland, I.:
Nonlinear oscillations and
boundary-value problems for Hamiltonian systems.
Arch. Rat. Mech. Anal. 78, 315--333 (1982)

\bibitem {clar:eke:2}
Clarke, F., Ekeland, I.:
Solutions p\'{e}riodiques, du
p\'{e}riode donn\'{e}e, des \'{e}quations hamiltoniennes.
Note CRAS Paris 287, 1013--1015 (1978)

\bibitem {mich:tar}
Michalek, R., Tarantello, G.:
Subharmonic solutions with prescribed minimal
period for nonautonomous Hamiltonian systems.
J. Diff. Eq. 72, 28--55 (1988)

\bibitem {tar}
Tarantello, G.:
Subharmonic solutions for Hamiltonian
systems via a $\bbbz_{p}$ pseudoindex theory.
Annali di Matematica Pura (to appear)

\bibitem {rab}
Rabinowitz, P.:
On subharmonic solutions of a Hamiltonian system.
Comm. Pure Appl. Math. 33, 609--633 (1980)

\end{thebibliography}

\section{OLD Background}
The study of machine intelligence has suffered from a number of false dawns, 
as practitioners at the forefront of the latest wave (dubbed 'deep learning') 
are keenly aware.  
%
%However, despite their caution, it remains difficult to restrain reporters
%who want to marvel at the latest 'magic' being produced by AI research.

In order to set the current wave in context, Joscha Bach \cite{Joscha-Bach-2012-interview}
commented that :

\begin{quotation}
%nature has managed to do it
%we don't know how the software should be written - 
%we have some good pointers though.

I think that eventually it will boil down to a few hundred organisational principals
\dots
%
%that can be encoded in the genome
%and the upper complexity boundary for 
%this is probably the information content of the genome
%which, famously, fits on a CDROM.
%
%So actually our blueprint for our body is simpler than
%the blueprint for a current edition of Microsoft Windows.  
%But it has not been built to be re-engineered.  
%
(However) it is very difficult to find the organising principles.

And it's often very easy to see them in hindsight : 
Backpropagation learning, with hindsight, is a very, very simple phenomenon, 
but it took mankind a very long time to hit upon the principle of Backpropagation learning. 

I guess we have a few hundred problems on the difficulty level of Backpropagation learning to solve before we get to AGI. 

%And so, in my perspective, we have many, many PhD students that need to get engaged
%with this and combine this into a common platform - I don't see any easy shortcuts.

\end{quotation}

This paper argues that several other building blocks are also evident, 
that backpropagation itself might be considered to be several related pieces of the puzzle,
and that current Machine Learning research, while exciting, may not be
discovering new blocks systematically.


\end{document}
