% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%  
\documentclass{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
%
\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads
\addtocmark{AGI Building Blocks} % additional mark in the TOC
%
\mainmatter              % start of the contributions
%
\title{AGI Building Blocks : The first 100 steps}
%
\titlerunning{AGI Building Blocks}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Martin Andrews\inst{1}}
%
\authorrunning{Martin Andrews} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Martin Andrews}
%
\institute{Red Cat Labs, Singapore, \\
\email{Martin.Andrews@RedCatLabs.com},\\ 
WWW home page: \texttt{http://RedCatLabs.com/}
}

\maketitle              % typeset the title of the contribution

%%%% using at least 70 and at most 150 words. It will be set in 9-point
\begin{abstract}
In an interview during AGI 2014, Joscha Bach made an interesting comment that 
building AGI would be the result of (ballpark) 100 different organizational principles - 
with Backpropagation being an example of a principle that could be on that list.  
This discussion paper outlines a few more candidate building blocks, 
while suggesting that Backpropagation is unlikely to be,  in itself, foundational.  
\dots
\keywords{building blocks, backpropagation, deep learning, hype}
\end{abstract}
%

\section{Background}
The study of machine intelligence has suffered from a number of false dawns, 
as practitioners at the forefront of the latest wave (dubbed 'deep learning') 
are all too aware.
However, despite their caution, it remains difficult to restrain reporters
who want to marvel at the latest 'magic' being produced by AI research.

In order to set the current wave in context, Joscha Bach 
( at AGI 2012 in Oxford 12m38s - 13m48s )  commented that :


\begin{quotation}
%nature has managed to do it
%we don't know how the software should be written - 
%we have some good pointers though.

I think that 
eventually it will boil down to a few hundred organisational principals
\dots

%that can be encoded in the genome
%and the upper complexity boundary for 
%this is probably the information content of the genome
%which, famously, fits on a CDROM.

%So actually our blueprint for our body is simpler than
%the blueprint for a current edition of Microsoft Windows.  
%But it has not been built to be re-engineered.  

It is very difficult to find the organising principles.

And it's often very easy to see them in hind-sight : 
Backpropagation learning, with hindsight, is a very, very simple phenomenon, 
but it took mankind a  very long time to hit upon the principle of Backpropagation learning. 

I guess we have a few hundred problems on the difficulty level of Backpropagation learning to solve before we get to AGI. 

%And so, in my perspective, we have many, many PhD students that need to get engaged
%with this and combine this into a common platform - I don't see any easy shortcuts.

\end{quotation}

This paper argues that several other building blocks are also evident, 
that backpropagation itself might be considered to be several related pieces of the puzzle,
and that current Machine Learning research, while exciting, may not be
discovering new blocks systematically.

\subsection{Overview}

Short "technical communications", with a limit of 4 pages, 
summarizing results and ideas of interest to the AGI audience, 
including : 
  reports about recent publications, 
  position papers, and 
  preliminary results.


Appropriate topics for contributed papers include, but are not restricted to:
  Creativity
  Knowledge Representation for General Intelligence
  Learning, and Learning Theory
  Natural Language Understanding
  Neural-Symbolic Processing
  Philosophy of AGI
  Reinforcement Learning

\subsubsection*{Back-Propagation.}
sdfksdkfldf asf asdf asdf asdf asdf 

\subsubsection*{Back-Propagation Again.}
sdfksdkfldf asf asdf asdf asdf asdf 

\paragraph{Notes and Comments.}

have obtained lower bound on the number of subharmonics of period $kT$,
based on symmetry considerations and on pinching estimates, as in
Sect.~5.2 of this article.


%
% ---- Bibliography ----
%
\begin{thebibliography}{5}
%


\bibitem {Bengio-et-al-2014-Book}
Yoshua Bengio and Ian J. Goodfellow and Aaron Courville:
Deep Learning.
Book in preparation for MIT Press (2014)
% Fix the tilde...
\newcommand{\mytilde}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}
% http://www.iro.umontreal.ca/~bengioy/dlbook/disentangling.html
(http://www.iro.umontreal.ca/\mytilde{}bengioy/dlbook)


\bibitem {clar:eke}
Clarke, F., Ekeland, I.:
Nonlinear oscillations and
boundary-value problems for Hamiltonian systems.
Arch. Rat. Mech. Anal. 78, 315--333 (1982)

\bibitem {clar:eke:2}
Clarke, F., Ekeland, I.:
Solutions p\'{e}riodiques, du
p\'{e}riode donn\'{e}e, des \'{e}quations hamiltoniennes.
Note CRAS Paris 287, 1013--1015 (1978)

\bibitem {mich:tar}
Michalek, R., Tarantello, G.:
Subharmonic solutions with prescribed minimal
period for nonautonomous Hamiltonian systems.
J. Diff. Eq. 72, 28--55 (1988)

\bibitem {tar}
Tarantello, G.:
Subharmonic solutions for Hamiltonian
systems via a $\bbbz_{p}$ pseudoindex theory.
Annali di Matematica Pura (to appear)

\bibitem {rab}
Rabinowitz, P.:
On subharmonic solutions of a Hamiltonian system.
Comm. Pure Appl. Math. 33, 609--633 (1980)

\end{thebibliography}

\end{document}
