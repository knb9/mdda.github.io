% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%  
\documentclass[citeauthoryear]{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
%
\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads
\addtocmark{Lessons from Deep Learning} % additional mark in the TOC
%
\mainmatter              % start of the contributions
%
\title{Lessons from Deep Learning}
%
\titlerunning{Lessons from Deep Learning}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Martin Andrews\inst{1}}
%
\authorrunning{Martin Andrews} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Martin Andrews}
%
\institute{Red Cat Labs, Singapore, \\
\email{Martin.Andrews@RedCatLabs.com},\\ 
WWW home page: \texttt{http://RedCatLabs.com/}
}

\maketitle              % typeset the title of the contribution

%%%% using at least 70 and at most 150 words. It will be set in 9-point
\begin{abstract}
%In an interview \cite{Joscha-Bach-2012-interview} during AGI 2012, Joscha Bach made an interesting comment that 
%the process of achieving AGI will likely require the discovery of its `100 organizational principles'.
%Those principles may be obvious in hindsight, but (using the example of Backpropagation learning)
%may take a long time for mankind to discover.
%This short communication outlines a few more candidate building blocks to put on the list, 
%while discussing which apects of Backpropagation mean that it deserves to be included.  

%The complexity of each organizational principle might be of the order of Backpropagation learning - 

%  What deep learning can tell us about finding organizational principles

In an interview during AGI \cite{Joscha-Bach-2012-interview}, Joscha Bach
made an interesting comment that the process of achieving 
AGI will likely require the discovery of `100 organizational principles' - 
using Backpropagation learning as an example of the scale of each one of those principles.  

This short paper reflects on the lessons to be learned from 
the recent successes of Deep Learning research and its relevance in terms 
of `the list' of AGI principles.

\keywords{deep learning, backpropagation, AGI organisational principles}
\end{abstract}
%
\section{Background}
% **  What deep learning can tell us about finding organizational principles ** 

Within the Neural Network (NN) field, 
recent advances in Deep Learning demonstrate better-than-human 
levels of perfomance in image classification, 
and dramatic improvements in domains such as speech recognition.  
This short paper reflects on the lessons that can be learned 
from the Deep Learning AI sub-genre, which has had its own `winter' 
since NNs were first fashionable in the 1980s.  

%\section{Overview}
%
%Short "technical communications", with a limit of 4 pages, 
%summarizing results and ideas of interest to the AGI audience, 
%including : 
%  reports about recent publications, 
%  position papers, and 
%  preliminary results.
%
%Appropriate topics for contributed papers include, but are not restricted to:
%  Creativity
%  Knowledge Representation for General Intelligence
%  Learning, and Learning Theory
%  Natural Language Understanding
%  Neural-Symbolic Processing
%  Philosophy of AGI
%  Reinforcement Learning
%
%Submission deadline moved from 15-march-2015 to 1-apr-2015
%  :: http://agi-conf.org/2015/call-for-papers/

\section{Deep Learning Lessons}

Here, for simplicity, the `early days' will refer to the 1980s/early-1990s,
the `new phase' as 2006 onwards, and `recently' to 2011 to the present day
(with much respect to those who founded the field earlier, and also 
those who soldiered on during the in-between times).  
For a comprehensive review of the NN and Deep Learning literature, see Schmidhuber (\cite{SchmidhuberOverview}).
 

\subsubsection*{Did we give up too early?}
 
The reasons being given for the success of the new phase of Deep Learning 
(compared to the early days) is the combination of increased model size and 
vast amounts of training data, using machines that are orders of magnitude faster.  
Considering each of these factors :

\begin{description}

\item[Model size] - in the early days, it was known (see Harpur (\cite{harpur-thesis})) 
that tractable techniques such as Principal Component Analysis (PCA) could be implemented 
in a biologically realistic setting, as well as via more efficient (for silicon) numerical implementations.  
Those linear-based models would have been tractable then, 
and could serve (even now) as a realistic baseline against which to compare 
more elaborate neural network models (Chan et al (\cite{PCAnet})).

\item[Data] - simply having more data available during the `new phase' 
is also a factor (The Unreasonable Effectiveness of Data (Halevy, Norvig \& Pereira, \cite{norvig-UnreasonableEffectivenessOfData})
- as is the bravado actually to push it through a network to see what happens.

\item[Processing Speed] - this factor would be more convincing if it had been reported at the time 
that the experiments were working, but much too slowly.  
In today's experiments, large (downward) jumps in error are experienced in early training epochs.  
These jumps should have been observable even if the experiment couldn't have 
been run to convergence in the past.  

So maybe the reasoning is more nuanced : It is not just the raw learning speed 
for a given experiment that is important, but 
(from a researcher's point of view) the iteration / experiment cycle time.

\end{description}


\subsubsection*{Random Propagation.}

While the basic backpropagation is easily understood as the application 
of the chain rule backwards through the network, it is rarely used in pure form
because various speed-up methods (such as momentum terms, ADAgrad, RMSprop, etc)
are almost a requirement for reasonable training times.  

While some of these methods have their roots in previous optimisation theory, 
others have been developed by `seeing what works' - and have shown that 
learning occurs even with random matrix error propagation 
(Lillicrap \cite{Lillicrap-random-matrix}).  
This robustness is a characteristic of `unfair advantage'.


\subsubsection*{Exploding gradients.}

The basic operation of a network layer (matrix multiply and non-linearity) 
naturally leads to the problem of `exploding gradients'.
With the new phase of research came techniques that mitigate the problem : 

\begin{description}

\item[Rectified Linear Units (ReLU) layers] - by computing output vectors $y$ 
as $y=max(Wx+b, 0)$, this minimalistic computation neatly avoids 
scaling problems by having the non-linearity agnostic to scale.

\item[Informed intialisation schemes] - network weights are initialised 
to random values that are scaled based on fan-in/fan-out considerations as a way of making the 
average scaling-factor of the matrix multiplication closer to unity.

\item[Normalization] - the scaling of the layer outputs is controlled
dynamically (described in G{\"u}l{\c{c}}ehre and Bengio, \cite{bengio-whitening} and 
Ioffe \& Szegedy, \cite{WhiteningOfData}).

\end{description}

In retrospect, the fact that simple Genetic Algorithms (GAs) could train 
NNs roughly as quickly as gradient descent during the early days, 
should have been telling : With exact gradients, local search should have 
been much more effective than any global search method.


\subsubsection*{Natural Problems aren't Antagonistic.}

In the early days, NN researchers spent a lot of time on the 
XOR problem, and with self-selected toy problems that were 
intentionally made challenging for networks to learn.

More recently, Deep Learning sucessess has shown that real-world data sets 
are not as antagonistic to being learned as might have been anticipated.  
This might illustrate that the priors embedded in network learning are 
somehow sympathetic to the data being learned 
(see, for instance, "Priors Regarding The Underlying Factors" in Bengio et al. \cite{Bengio-et-al-2014-Book}),
or that the interesting features that humans themselves have identified as being 
worth learning are those that `come easily' to human brains.
\footnote{
Label sparsity is something that likely occurs at multiple levels in the brain 
(rather than just at the output layer) and is also in contrast to the 
nuanced internal representations that a simple PCA approach might suggest.
}

\subsubsection*{Common Problem Sets.}

Introducing a competitive element into research, by having fully published
training and test sets, has clearly pushed the state-of-the-art along more 
quickly than the previous regime in which interesting data sets were closely held.
The flip side of this is that less time is spent on understanding 
compared to the discovery of radical ideas and subsequent model refinement.


\section{Deep Learning Discoveries}

The following can all be characterized as `unfairly effective', and have 
arisen as part of the rapid exploration that characterises Deep Learning research.

\subsubsection*{Trailblazing.}
The Deep Learning field appears wide open to new ideas.  
Practitioners seem to feel free attempt the impossible.  
This is sometimes reincorporated into the mainstream once researchers in those 
fields realize that something previously unthinkable actually works.

\subsubsection*{Transfer Learning / Internal Representations.}
Taking a cross-section through a trained network to see its internal representation
has been shown to be applicable to other problem domains (Karpathy \& Fei-Fei, \cite{karpathy2014deep}).

The usefulness of these internal representations
\footnote{ ... which could also be thought of as 'ideas', particularly 
if there were a means to capture transitions between them, aka `stories'.}
has powerful implications regarding the power of networks trained by 
different modalities to enhance each other's effectiveness without the 
brittleness associated with building a single 'monolithic' system.

\subsubsection*{Word Vector Embedding.}
Originally almost a by-product of learning a word-prediction task, a vector 
space can be automatically learned to describe words (given a sufficient
quantity of well-formed sentences in a given langauge, and \emph{zero} linguistic knowledge).
This is both surprising and encouraging for AGI.

\subsubsection*{Convolutional NNs.}
This formulation of NNs started as a trick to usefully capture 
the structural properties of input images.  
However, it was then found that the network layers map (loosely) 
to the different levels of processing within the brain in a way that is highly suggestive 
of having explanatory power.

\subsubsection*{Drop-out.}
This training enhancement (Hinton et al, (\cite{hinton-dropout})) 
can be rationalised as allowing multiple competitive ensembles to self-organize.  
However, despite the essence of its operation not being fully understood, 
many researches are using it as part of the standard toolkit without 
exploring why it works.


\section{Other Organisational Principles}

As a side-note, there are a few more organisational principles that seem appropriate 
to label `foundational' to the cause of AGI :
\begin{description}
\item[Ensemble Methods] - whereby an ensemble of different approaches to 
solving a problem to rise above the accuracy 
of any one method has a simple mathematical underpinning.  
\item[Sparse Encodings] - relatively unexplored compared to the use that 
the brain appears to make of them (note that Sparse PCA, while interesting, 
may be optimising for the `final layer' rather than useful intermediate steps).
\item[Evolutionary Methods] - currently appear to be suffering through 
a winter of its own, despite working `unfairly well'.
\item[Minimum Description Length] - a concept-cluster that extends from compression, 
to information theory, encompassing Occam's razor along the way.
\end{description}

%\section{Conclusions}

%
% ---- Bibliography ----
%
\begin{thebibliography}{}
%
% Fix the tilde...
\newcommand{\mytilde}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}

\bibitem[2012]{Joscha-Bach-2012-interview}
Joscha Bach:
Interview during AGI 2012.
Available on Youtube, 12m38s - 13m48s (2012)

\bibitem[2014]{Bengio-et-al-2014-Book}
Yoshua Bengio and Ian J. Goodfellow and Aaron Courville:
Deep Learning.
% http://www.iro.umontreal.ca/~bengioy/dlbook/disentangling.html
(http://www.iro.umontreal.ca/\mytilde{}bengioy/dlbook)
MIT Press (2014, book in preparation)

\bibitem[2014]{PCAnet}
Tsung-Han Chan, Kui Jia, Shenghua Gao, Jiwen Lu, Zinan Zeng, Yi Ma:
PCANet: A Simple Deep Learning Baseline for Image Classification?
eprint arXiv:1404.3606v2 (2014)

\bibitem[2013]{bengio-whitening}
G{\"u}l{\c{c}}ehre, {\c{C}}a{\u{g}}lar and Bengio, Yoshua:
Knowledge matters: Importance of prior information for optimization.
CoRR, abs/1301.4083 (2013)

\bibitem[2009]{norvig-UnreasonableEffectivenessOfData}
Alon Halevy, Peter Norvig, Fernando Pereira:
The Unreasonable Effectiveness of Data.
IEEE Intelligent Systems, vol. 24, pp. 8-12 (2009)

\bibitem[1997]{harpur-thesis}
Harpur, George Francis: 
Low entropy coding with unsupervised neural networks.
Dissertation, University of Cambridge (1997)

\bibitem[2012]{hinton-dropout}
Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R:
Improving neural networks by preventing co-adaptation of feature detectors.
arXiv preprint arXiv:1207.0580 (2012)

\bibitem[2015]{WhiteningOfData}
Sergey Ioffe, Christian Szegedy:
Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.
eprint arXiv:1502.03167v3 (2015)

\bibitem[2014]{karpathy2014deep}
Karpathy, Andrej and Fei-Fei, Li:
Deep visual-semantic alignments for generating image descriptions.
arXiv preprint arXiv:1412.2306 (2014)

\bibitem[2014]{Lillicrap-random-matrix}
Lillicrap, Timothy P.; Cownden, Daniel; Tweed, Douglas B.; Akerman, Colin J.:
Random feedback weights support learning in deep neural networks.
eprint arXiv:1411.0247 (2014)

\bibitem[2014]{SchmidhuberOverview}
J{\"u}rgen Schmidhuber:
Deep Learning in Neural Networks: An Overview.
eprint arXiv:1404.7828v4 (2014)

%\bibitem {clar:eke}
%Clarke, F., Ekeland, I.:
%Nonlinear oscillations and
%boundary-value problems for Hamiltonian systems.
%Arch. Rat. Mech. Anal. 78, 315--333 (1982)
%
%\bibitem {clar:eke:2}
%Clarke, F., Ekeland, I.:
%Solutions p\'{e}riodiques, du
%p\'{e}riode donn\'{e}e, des \'{e}quations hamiltoniennes.
%Note CRAS Paris 287, 1013--1015 (1978)
%
%\bibitem {mich:tar}
%Michalek, R., Tarantello, G.:
%Subharmonic solutions with prescribed minimal
%period for nonautonomous Hamiltonian systems.
%J. Diff. Eq. 72, 28--55 (1988)
% 
%\bibitem {tar}
%Tarantello, G.:
%Subharmonic solutions for Hamiltonian
%systems via a $\bbbz_{p}$ pseudoindex theory.
%Annali di Matematica Pura (to appear)
%
%\bibitem {rab}
%Rabinowitz, P.:
%On subharmonic solutions of a Hamiltonian system.
%Comm. Pure Appl. Math. 33, 609--633 (1980)

\end{thebibliography}

\newpage

\section{OLD Background}
The study of machine intelligence has suffered from a number of false dawns, 
as practitioners at the forefront of the latest wave (dubbed `deep learning') 
are keenly aware.  
%
%However, despite their caution, it remains difficult to restrain reporters
%who want to marvel at the latest `magic' being produced by AI research.

In order to set the current wave in context, Joscha Bach \cite{Joscha-Bach-2012-interview}
commented that :

\begin{quotation}
%nature has managed to do it
%we don't know how the software should be written - 
%we have some good pointers though.

I think that eventually it will boil down to a few hundred organisational principals
\dots
%
%that can be encoded in the genome
%and the upper complexity boundary for 
%this is probably the information content of the genome
%which, famously, fits on a CDROM.
%
%So actually our blueprint for our body is simpler than
%the blueprint for a current edition of Microsoft Windows.  
%But it has not been built to be re-engineered.  
%
(However) it is very difficult to find the organising principles.

And it's often very easy to see them in hindsight : 
Backpropagation learning, with hindsight, is a very, very simple phenomenon, 
but it took mankind a very long time to hit upon the principle of Backpropagation learning. 

I guess we have a few hundred problems on the difficulty level of Backpropagation learning to solve before we get to AGI. 

%And so, in my perspective, we have many, many PhD students that need to get engaged
%with this and combine this into a common platform - I don't see any easy shortcuts.

\end{quotation}

This paper argues that several other building blocks are also evident, 
that backpropagation itself might be considered to be several related pieces of the puzzle,
and that current Machine Learning research, while exciting, may not be
discovering new blocks systematically.

The history of AI research shows that the discovery of new techniques 
oftne appears to deamatically expand the boundaries of what is 
tractable by computer methods.  
However, AI has been repeatedly cursed by hype, whereby the problems 
solved are soon discovered to be `low-hanging fruit' compared to subsequent 
hurdles in the overall AGI endeavour.



\subsubsection*{Ensemble Methods.}
The ability of an ensemble of different approaches to solving a problem to rise above the accuracy 
of any one method has a simple mathematical underpinning.  

Can't make an ensemble with single learned core - no diversity created

drop-out is an interesting technique that deserves to be investigated in itself
  possible to use `structured' dropout that is more compute-friendly?


\subsubsection*{Sparse Encodings.}
  relatively unexplored, compared to the use that the brain appears to make of it
    may be an artifact of brain's components
    
\subsubsection*{Evolution.}
  Works `unfairly well'
  Distance from computation
    GAs operate directly (parameters for a given substrate)
    GPs operate (parameters that directly create a substrate)
    DNA : code that creates stuff that self-organizes to create conditions where 
      other parts of DNA interact, to produce...   
        final result is third / forth hand from inital representation

\subsubsection*{Minimum Description Length.}
This concept-cluster is another mathematical one : extending from compression, to information theory,
encompassing Occam's razor along the way.

\subsubsection*{Stories.}
  ???


\section{BackPropagation Thoughts}
  gradient descent is just a method for energy minimisation
    there are lots of interesting speed-ups (momentum, AGAgrad, etc)
      but biological plausibility already left the building
      
    actual goal is finding lower-energy states
      update methods and enhancements are techniques, not essential elements




The robustness of the quasi-backpropation algorithms being used to noise
suggests that brute-force computation can beat elegant mathematics, and that 
the brain doesn't need to be able to implement an optimal strategy - just one that works.


unsupervised learning is actually weakly supervised 

  humans perceive objects that have relationships and labels on different levels
    these tasks are implicitly also human-learnable
    properties that are useful to neural network learning (priors, causality, shared factors) are embedded already
    
  no interest in classifying (for instance) different `random noise' images
  
  features that are interesting to humans seem to be either fairly independent or subsetting
    unlikely to have interesting features that are complicated vectors of properties
    this is an interesting prior that can be imposed on a network

\subsubsection*{Convolutional Nets.}
The formulation of Convolution Networks is also part of the new movement, 
and it is interesting because it exposes a useful representational property of images.  
But it is somewhat hiding the fact that human knowledge is being implicitly 
built into the network via the representation scheme.

The flip side of this is that the convolutional layers seem to 
map (loosely) to the different levels of processing within the brain - 
in a way that is highly suggestive of having explanatory power.


\section{The Known Unknown}

conciousness 
  still no idea where to start
  difference between making a dog and making a person
    capability for sophisticated language model
      markov state + RNN + beam search == plausible linguistic output
  
  stories that we tell ourselves
    narrative that we are aware of is highly edited version of sub
      `top of mind' is what floats to the surface
      deep sea of unconscious though seething away beneath
  introspection necessarily can't get beneath the surface

 are not as clear-cut as they might first appear,
because the field is still in a rapid exploration phase, and the `conventional wisdom' is
being rewritten regularly.  

In addition, away from the lime-light surrounding 
the production of headline results for competitive benchmarks, 
there may still be much to be gained from examining some of 
the field's basic questions.

\end{document}


