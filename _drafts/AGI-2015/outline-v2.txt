

back-propagation : what it is, and what it isn't
  gradient descent is just a method for energy minimisation
    there are lots of interesting speed-ups (momentum, AGAgrad, etc)
      but biological plausibility already left the building
    actual goal is finding lower-energy states
      update methods and enhancements are techniques, not essential elements
      
  many other methods exist for minimisation
    eg: WTF-propagation : random matrix error propagation (Lillicrap 2014)
      possible to use 'structured' random matrices that are compute-friendly?
      suggests that brute-force computation beats elegant mathematics
        brain doesn't need to implement best strategy - just one that works
      
  looking at, for instance, whitening : this is a bigger win than fancier gradient descent methods
    shows that engineering for efficiency may be missing bigger picture
    could whitening applied to old-generation models actually work?

  drop-out is an interesting technique that deserves to be investigated in itself
    possible to use 'structured' dropout that is more compute-friendly?
    
Deep Learning : 
  New crop of low-hanging fruit 
    ref: previous AI hype that lead to winters after picking the low-hanging fruit

ensemble methods
  this has a direct mathematical underpinning
  can't ensemble with single learned core - no diversity created
  
sparse encodings
  relatively unexplored, compared to the use that the brain appears to make of it
    may be an artifact of brain's components
    but, looking at word-embedded (whether as float-vectors, or as coincidence matrices)
      sparse pattern matching is more 'unfairly' powerful 
      random projection

Evolution
  Works 'unfairly well'
  Distance from computation
    GAs operate directly (parameters for a given substrate)
    GPs operate (parameters that directly create a substrate)
    DNA : code that creates stuff that self-organizes to create conditions where 
      other parts of DNA interact, to produce...   
        final result is third / forth hand from inital representation

internal representations
  cross-domain knowledge
    already demonstrated that internal representations can be re-used effectively
    another 'unfair advantage'

higher level human understanding of :
  mathematics
    energy minimisation
    minimum description length
    mutual information
    
  
  systems thinking
    hierarchies of concepts
      system goals of brain/neurons not necessarily optimal for conciousness
    can be a trap 
      'thoughts' do can flow fluidly - no need for logical construction
        in retrospect, logic can be found
      mass experimentation (at forefront of Deep Learning, for instance) beneficial
        things that work unfairly well can be systematised/rationalised later


  symbolic reasoning
    brain has a single substrate, organized 'third hand' by DNA
    AGI designers operate more hands-on
      constrained by iteration speed (new structures)
      constrained by local search (new ideas)


system goals of brain/neurons not necessarily optimal for conciousness
  brain 
    robustness
    lower power
    requirements of modern society (much higher reasoning ability) too recent for evolution
    3d structure
    approximate replicability
    wasteful approaches to computation may still be a 'win' in other aspects 
      (power, speed, robustness, style of convergence, communicability, etc)
    limited capacity for 'front of mind' ideas : 6 things
    
  machine
    repeatability
    precision (available)
    ability to make use of power-wasteful structures (can re-engineer heat flow)
    

conciousness 
  still no idea where to start
  difference between making a dog and making a person
    capability for sophisticated language model
      markov state + RNN + beam search == plausible linguistic output
  
  stories that we tell ourselves
    narrative that we are aware of is highly edited version of sub
      'top of mind' is what floats to the surface
      deep sea of unconscious though seething away beneath
  introspection necessarily can't get beneath the surface

unsupervised learning is actually weakly supervised 
  all training examples are positive (compared to non-training examples)
  JPEG is good compression of visual images
    suggests that we might train on that, since it's already semi-optimised for visual field

organisation of interesting test cases is biased
  humans perceive objects that have relationships and labels on different levels
    these tasks are implicitly also human-learnable
    properties that are useful to neural network learning (priors, causality, shared factors) are embedded already
  no interest in classifying (for instance) different 'random noise' images
  features that are interesting to humans seem to be either fairly independent or subsetting
    unlikely to have interesting features that are complicated vectors of properties
    this is an interesting prior that can be imposed on a network
      "Priors Regarding The Underlying Factors" : Section 14.5 of Bengio deep learning book (in preview) 
        \cite{Bengio-et-al-2014-Book} 

PCA is a 'high level' concept
  but can be implemented using simple circulation of error terms
  efficient compute implementation uses features of silicon unavailable to biological neurons


fertle / toofer / turst exercise in common features biological botanical
  where did I see this?
  


