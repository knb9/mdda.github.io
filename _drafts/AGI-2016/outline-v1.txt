
Consciousness and the Social Machine

Contributions

	Map current state-of-the-art to components of the Attention Schema theory
	Describe how clarity of this mechanism allows for pause/restore
	Point out the gap between requiring consciousness and (beyond) self-direction towards improvement


Describe the Attention Schema theory 

Attention Schema theory. The theory suggests that consciousness is no bizarre byproduct – it’s a tool for regulating information in the brain.

https://aeon.co/essays/can-we-make-consciousness-into-an-engineering-problem ::
"""
We know that, but it doesn’t. It possesses no information about how it was built. 
Its internal models don’t contain the information: ‘By the way, we’re a computing device that accesses internal models, and our models are incomplete and inaccurate.’ 
It’s not even in a position to report that it has internal models, or that it’s processing information at all.
"""
 
"""
The machine is captive to its internal models, so it can’t arrive at any other conclusions.
"""


Graziano still writing about his 'new theory' to promote it in popular press.


Vision CNN achievements

Attention mechanisms replacing state-transfer in LSTMs

Captioning indicates that 'internal process' could be converted into prose

Questions about image (or text) can be examined using their map of attention too


Reinforcement learning sucesses have required the bulding of a model
of the opponent - although this is simplified by the assumption of mini-max actions

  Hidden state complicate this - though there still are (?) optimal strategies, even in the face of uncertainty


Building a model of another actor is becoming (?) a necessasary feature of chatbot 
Self-image can be built on same basis


Empathy due to time-sharing of mental machinery in order to step into someone else's shoes


Callibrated consciousness
  Based on realism of models of 'other'
  Depth of introspection can be naturally limited / ascertained
  
  Ability to pause/save/restart 'self'
  Might simplify questions of 'rights'


Self image could also include honest description of differences from humans
  including an acceptance of being self-contained (and paused, etc)


Lower hurdle than self-improving AGIs
  No need to create novelty in decision-making policy, for instance
  
Doesn't need full canvas of inputs (compare to LIDA, for instance)
  The LIDA Model as a Foundational Architecture for AGI - http://ccrg.cs.memphis.edu/assets/papers/2012/LIDA-Foundational%20Architecture%20for%20AGI.pdf



Turing test involves convincing a human that a machine should be accredited with conciousness
  Which is a lower bar than that the machine actually has conciousness
  OTOH, if the workings of the machine can be fully analysed, many humans would reject the machine's consciousness because it appears insufficient

The machine reporting on it's own consciousness would lie on a spectrum from lying to honest to super-honest, 
  depending on the sophistication of its self mental model (and its capacity to model others could be measured)
  Potentially, the 'other' models could be more sophisticated than human ones
  
Self-model should be 'relateable' in human terms to aid recognition 
  
  There was a noticable shift in the way people related to AlphaGo's strategy once they understood that it was motivated by win-probability only
    Just having a glimpse into it's internal mindset gave commentators a way to relate to the machine (which they appeared to fiind reassuring)
    

Conclusions / Further Work

Improving chatbot behaviour appears to give the greatest utility to developing models of the attention / motivations of actors
  These models could be benchmarked - and competitions held (specifically for the representations built)
  Without that component, bots will struggle to relate well (or even understand the dialogue properly)
  Benefit in human-relatability would be a direct result
    That positive element of empathy has little bearing on machines being able to acheive super-human levels of performance on other tasks

