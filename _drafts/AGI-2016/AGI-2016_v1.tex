% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%  
\documentclass[citeauthoryear]{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
%
\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads
\addtocmark{Consciousness and the Social Machine} % additional mark in the TOC
%
\mainmatter              % start of the contributions
%
\title{Consciousness and the Social Machine}
%
\titlerunning{Consciousness and the Social Machine}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Martin Andrews\inst{1}}
%
\authorrunning{Martin Andrews} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Martin Andrews}
%
\institute{Red Cat Labs, Singapore, \\
\email{Martin.Andrews@RedCatLabs.com},\\ 
WWW home page: \texttt{http://RedCatLabs.com/}
}

\maketitle              % typeset the title of the contribution

%%%% using at least 70 and at most 150 words. It will be set in 9-point
\begin{abstract}



\keywords{consciousness,attention schema,attention,chatbots}
\end{abstract}
%
\section{Introduction}


%\section{Overview}
%
%Short "technical communications", with a limit of 4 pages, 
%summarizing results and ideas of interest to the AGI audience, 
%including : 
%  reports about recent publications, 
%  position papers, and 
%  preliminary results.
%
%Appropriate topics for contributed papers include, but are not restricted to:
%  Creativity
%  Knowledge Representation for General Intelligence
%  Learning, and Learning Theory
%  Natural Language Understanding
%  Neural-Symbolic Processing
%**Philosophy of AGI
%  Reinforcement Learning
%
%Submission deadline moved from 15-march-2015 to 1-apr-2015
%  :: http://agi-conf.org/2015/call-for-papers/


\section{Conclusions}




%
% ---- Bibliography ----
%
\begin{thebibliography}{}
%
% Fix the tilde...
\newcommand{\mytilde}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}

%\bibitem[2012]{Joscha-Bach-2012-interview}
%Bach, Joscha:
%AGI 2012 Interview.
%(https://youtu.be/PyKzO0MF1zI?t=12m36s)

\bibitem[2014]{Bengio-et-al-2014-Book}
Bengio, Yoshua; Goodfellow, Ian J. and Courville, Aaron:
Deep Learning.
% http://www.iro.umontreal.ca/~bengioy/dlbook/disentangling.html
(http://www.iro.umontreal.ca/\mytilde{}bengioy/dlbook)
MIT Press (2014, in prep.)

%\bibitem[2014]{PCAnet}
%Tsung-Han Chan; Kui Jia; Shenghua Gao; Jiwen Lu; Zinan Zeng; Yi Ma:
%PCANet: A Simple Deep Learning Baseline for Image Classification?
%arXiv:1404.3606v2 (2014)

%\bibitem[2013]{bengio-whitening}
%G{\"u}l{\c{c}}ehre, {\c{C}}a{\u{g}}lar and Bengio, Yoshua:
%Knowledge matters: Importance of prior information for optimization.
%CoRR, abs/1301.4083 (2013)

%\bibitem[2009]{norvig-UnreasonableEffectivenessOfData}
%Halevy, Alon; Norvig, Peter and Pereira, Fernando:
%The Unreasonable Effectiveness of Data.
%IEEE Intelligent Systems, vol. 24, pp. 8-12 (2009)

%\bibitem[1997]{harpur-thesis}
%Harpur, George Francis: 
%Low entropy coding with unsupervised neural networks.
%Dissertation, University of Cambridge (1997)

%\bibitem[2012]{hinton-dropout}
%Hinton, Geoffrey E; Srivastava, Nitish; Krizhevsky, Alex; Sutskever, Ilya and Salakhutdinov, Ruslan R:
%Improving neural networks by preventing co-adaptation of feature detectors.
%arXiv:1207.0580 (2012)

%\bibitem[2015]{WhiteningOfData}
%Ioffe, Sergey and Szegedy, Christian:
%Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.
%arXiv:1502.03167v3 (2015)

\bibitem[2014]{karpathy2014deep}
Karpathy, Andrej and Fei-Fei, Li:
Deep visual-semantic alignments for generating image descriptions.
arXiv:1412.2306 (2014)

%\bibitem[2014]{Lillicrap-random-matrix}
%Lillicrap, Timothy P.; Cownden, Daniel; Tweed, Douglas B. and Akerman, Colin J.:
%Random feedback weights support learning in deep neural networks.
%arXiv:1411.0247 (2014)

\bibitem[2014]{SchmidhuberOverview}
Schmidhuber, J{\"u}rgen:
Deep Learning in Neural Networks: An Overview.
arXiv:1404.7828v4 (2014)

\end{thebibliography}

\end{document}


