---
layout: post
category: AI
title: Trailblazing
tagline: Mavericks Required
date: 2014-11-06
tags: [NLP,NeuralNetworks,PCA]
published: false
---
{% include JB/setup %}


Recently, I've notice a pattern of papers that show that 'simplistic methods' actually work - and are competitive with the ground-breakingly good results from the NN deep learning community.  For example : 

*  PCAnet vs cA, etc  [PCANet: A Simple Deep Learning Baseline for Image Classification?](http://arxiv.org/abs/1404.3606)

<!--
  Definitely need to contact authors in SG!  
    http://mx.nthu.edu.tw/~tsunghan/index.html
      SENT MAIL TO :: Tsung-Han Chan <thchan@ieee.org>
      http://scholar.google.com/citations?user=WDJ7tY0AAAAJ
      (now at SunPlus.com)
    SENT MAIL TO :: Jiwen.Lu@adsc.com.sg  << Only one on ADSC page...  (Referred back to orig author)24
!-->

*  single layer nets (trained from deeper nets)  [Do Deep Nets Really Need to be Deep?](http://arxiv.org/pdf/1312.6184.pdf)


*  GloVE vs hinton tree training [GloVe - Global Vectors for Word Representation - (Pennington, Socher, Manning 2014)](http://nlp.stanford.edu/pubs/glove.pdf)

*  incidence-based 'PCA' embedding vs GloVE  [Neural Word Embedding
as Implicit Matrix Factorization - Levy &amp; Goldberg (2014)](https://levyomer.files.wordpress.com/2014/09/neural-word-embeddings-as-implicit-matrix-factorization.pdf)  and [Word Emdeddings through Hellinger PCA - Chan et al (2014)](http://arxiv.org/abs/1312.5542)

### The Need for Trailblazers

