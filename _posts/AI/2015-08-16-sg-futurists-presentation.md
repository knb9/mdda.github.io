---
layout: post
category: AI
title: The Future of Mind - Talk at the SG Futurists MeetUp
tagline: Presentation link
date: 2015-08-16
tags: [NeuralNetworks,GOFAI,Presentation]
published: false
---
{% include JB/setup %}

### Presentation Notes

#### MeetUp Description Text

The talk will provide :

*  an overview of concepts and themes in Artificial Intelligence, 
     + including a review of the field today (notably deep learning) 
    
*  his perspectives on Artificial General Intelligence.


#### Presentation Outline


---
Front Page

---
About Me

---
The past

*  Chess  (1950s - 1997)
*  Jeopardy  (2004 - 2011)
*  Driverless cars  (1980s - present)

___
Chess

*  1957 - Prediction : Computer to be World #1 'within 10 years' 
*  1978 - First game win against master
*  1997 - First match win against #1 (Kasparov)
    -   DeepBlue : 200MM moves per second
*  2009 - Grandmaster level on your phone
    -   'Pocket Fritz' : 20k moves/sec

___
Jeopardy

+  2004 - New problem sought
+  2008 - IBM approached show hosts 
+  2011 - IBM Watson wins 
   -   "4Tb of disk storage"
   -   ADD IMAGE

___
Driverless cars

+  1980s - early work (CMU and Germany)
+  2004  - DARPA Grand Challenge - 12km furthest
+  2005  - DARPA Grand Challenge - 90% of contestents > 12km
+  2009  - Google Car driving autonomously around CA
+  2015  - Google Car : 
   -   Driven 2 million miles 
   -   14 minor accidents (caused by humans)
   
   
---
Current cycle focussing on Deep Learning

*  Neural Networks with many layers
   -   Old idea, came back to life

*  Specific 'branding' by leading lights (Hinton, Bengio, Le Cun, Andrew Ng) in mid 2000s
  -   http://spectrum.ieee.org/automaton/robotics/artificial-intelligence/facebook-ai-director-yann-lecun-on-deep-learning

___
"Neural"

*   One Layers

___
"Neural Networks"

*   Several Layers

___
"Deep Neural Networks"

*   Many Layers


---
It's Different this Time

*  Data : More useful than realised
*  GPUs : Turning point
*  New thinking

___
Data : More useful than realised

*    &gt;1Bn Photos per day 
    -    graph (http://www.kpcb.com/internet-trends)
*    Wikipedia (~6Gb) standard traing unit
*    Internet growth beating Moore's Law


___
GPU : Turning point

*    1000s of cores
*    &gt;1 GFLOP per dollar
*    1 week x 1000 



___
New thinking

*    Sigmoids &rarrow; ReLUs
*    Dropout (worse is better)
*    Batch normalization 
*    Networks for the whole stack



---
What can be done now

*  Audio to Text
*  Language translation 
*  Vision 
   -   Object recognition
   -   Automatic captioning
*  Reinforcement Learning


___
Audio 

*   In Android Jellybean (=4.3)
       -   http://www.phonearena.com/news/The-secret-of-Googles-amazing-voice-recognition-revealed-it-works-like-a-brain_id39938
*   Voice search works in offline mode in Android Lolipop (=5.0)
       -   http://android.stackexchange.com/questions/100492/offline-voice-recognition-not-working 
   
*   Also
       -   http://www.androidheadlines.com/2014/10/google-talk-neural-networks-voice-recognition.html
       -   http://googleresearch.blogspot.sg/2015/08/the-neural-networks-behind-google-voice.html

___
Translation 

*   google translate 
       -   compressed blog posting confirms availability :: http://googleresearch.blogspot.sg/2015/07/how-google-translate-squeezes-deep.html
       -   has nice image
       -   ADD IMAGE

___
Vision 

*   google house numbers
       -   Better-than human :: http://www.geek.com/news/googles-neutral-networks-are-now-better-than-humans-at-reading-addresses-1581653/
       -   http://arxiv.org/abs/1312.6082
       -   ADD IMAGE
___
Vision 

*   ImageNet
       -   Human-competitive (partly to do with dataset)
       -   ADD IMAGE

___
Captioning Images 

*   Google Photos (now)
*   Captioning example
       -   ADD IMAGE (Karpathy)


___
Reinforcement Learning 

*   DeepMind Atari
       -   ADD IMAGE


---
"AI Effect"

*  "AI Effect" : https://en.wikipedia.org/wiki/AI_effect
   -   AI is whatever hasn't been done yet



---
Commercial Side

*   It feels like...  'X' wants AI to 'Y' 

    -   IBM : Sell to Enterprise 
    -   Apple : be Magical
    -   Baidu : be Functional
    -   Facebook : be your Friend
    
    -   Google : Happen

---
How far can Deep Learning take us?

*   lots of fruit now found to be 'low hanging'
    -   personal experience : Grammar as a Language => DeepNER


*   Trap : Building ever taller ladders when goal is to reach the Moon


*   Higher principals are more important
    -   Aeroplanes don't flap wings 
    -   What is the analog of aerodynamics for intelligence?



---
Futurism

*   Graph of silicon vs brain compute power (omitted)

*   Singularity worries
    -   Perhaps AI workers arrive in time to difuse the demographic time-bomb


*   Arrival of Smart machines
    -   To get to AI we'll pass through:
        +    spider, mouse, dog
        +    Homer Simpson, Bart Simpson, Lisa Simpson
        +    Einstein, etc
    -   Not clear that twice the capacity => twice the smarts


---
How about Strong AI?

*   Bound to be speculative
*   Don't even know the right questions to ask
*   Compare "Science of Love" to "Science of Conciousness"



---
Basic philosophical ideas --  Not the meat of the talk...

*   Is "Real AI" actually possible ?

*   How do we measure progress / success ?


___
Magic and the Brain

*   Special magic
    -   homunculus : The real 'you'
*   One neuron at a time
    -   In-principle idea
*   'Not the same'
    -   Does a submarine swim?


___
Searle's Chinese Room

*   Description
    -   Arguments
*   System (vs operator) intelligence 



___
Turing Test

*   Description

*   Cheating is now the issue
    -   Probably more like a dinner party conversation
    
*   Looking for what a human does that a dog doesn't

    

---
Problems with studying Conciousness

*   Difficult to dig deeply into the actual process of thought
*   People look to 
    -   brain damage
    -   disorders 
    -   visual tricks
    -   child development
*   Conscious thought is the water's surface, whereas mind is the ocean



---
Brain Substrate

*   Substrate for brain may be poor for 'higher order thinking'
    -   Depth and width

*   Neural mechanisms are also tailored for 'real world'
    -   categorisation / hierarchies
    -   activities designed to fit with our thinking
    
*   Language is pure human construct - optimised for transmission of ideas
    -   not generated by nature (like images, which brains have adapted to)
    -   more compressed data stream : Minimal hinting



---
'Thought process' of AI likely to be very different

*   Simple vs Hard
    -   people whose names have 'M' as first letter 
    -   people whose names have 'R' as third letter

*   Different algorithms
    -   page numbering

*   Incompatible hardware 
    -   Uploading doesn't make sense to me
    


---
Searching for Higher Principals

*   Using Biological neurons:
    -   accept that brain solves problems efficiently
    -   but that doesn't mean it is best way

*   What are the actual take-away from current successes?

*   What actual problems can be solved more abstractly?


___
Stuff that works better-than-expected

*   Convolutional networks for images
*   Word Embedding
*   Transfer learning 
     -   Learning one task can 'cross-fertilize'

___
Stuff that can be abstracted

*   Hebbian Learning
    -   "cells that fire together wire together"
    -   Gradient descent
    -   Optimisation 
        -   Matrix Methods
        -   Genetic Algorithms


___
Candidates for Higher principals 

*   Learning from data

*   Compression of Experience
    -   Minimal Description Length
    -   Occam's razor

*   Sparse Representations

*   Bayesian statistics

*   Search for Novelty / Surprise

*   Language itself is revealing
    -    highly evolved protocol


---
Conclusions

*   Great strides being made
*   AI Effect is a problem
    +   Don't even know the right questions to ask
*   Deep Learning frontier is being pushed by competitions
    +   Need to figure out competitive 'thinking' data/interactions



{% comment %} 
### Presentation Link

I recently gave a <strong><a href="http://redcatlabs.com/2015-06-19_Presentation-PyConSG/" target="_blank">presentation about Python, Theano, Blocks and Deep Learning</a></strong> 
at the [2015 Singapore PyCon](https://pycon.sg/).

![Presentation Screenshot]({{ site.url }}/assets/img/2015-06-19_Presentation-PyCon_600x390.png)

If there are any questions about the presentation, please ask below - 
alternatively, have a look at my [GitHub repo](https://github.com/mdda/pycon.sg-2015_deep-learning) 
that contains all the iPython notebooks, code and documentation that I put together
in preparation for the talk.

![Presentation Content Example]({{ site.url }}/assets/img/2015-06-19_Presentation-PyCon_8-5_600x390.png)
{% endcomment %} 
